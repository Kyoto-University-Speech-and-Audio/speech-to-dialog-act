{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "TEST_DATA_FOLDERS = [\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/interview\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/dating\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/attentive\"\n",
    "]\n",
    "TRAIN_DATA_FOLDERS = [\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/interview\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/dating\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/attentive\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto16/interview\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto16/dating\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto16/attentive\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto16/labintro\",\n",
    "]\n",
    "\n",
    "mode = \"train\"\n",
    "\n",
    "from subprocess import call\n",
    "from struct import unpack, pack\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import random\n",
    "\n",
    "import MeCab\n",
    "mt = MeCab.Tagger(\"-Owakati\")\n",
    "mt.parse('')\n",
    "\n",
    "OUTPUT_FOLDER = \"/n/sd7/trung/csp/data/erica\"\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30638\n"
     ]
    }
   ],
   "source": [
    "words = [s.strip().split(' ', 1) for s in open('/n/rd32/mimura/e2e/data/script/aps_sps/word.id', encoding='eucjp')]\n",
    "decoder_map_word = {word[0].split('+')[0]: int(word[1]) for word in words}\n",
    "\n",
    "# words = [s.strip().split(' ', 1) for s in open('/n/sd7/trung/csp/data/erica/word_ids.txt') if s != \"\"]\n",
    "# decoder_map_word = {word[0]: int(word[1]) for word in words}\n",
    "# decoder_map_word = {'<unk>': 0, '<sos>': 1, '<eos>': 2, '<sp>': 3}\n",
    "print(\"Vocab Size:\", len(decoder_map_word))\n",
    "\n",
    "TAG_COUNT = 17\n",
    "def get_tag_id(tag):\n",
    "    for i, t in enumerate(['pQ', 'cQ', 'sQ', 'checkQ', 'inf', 'off', 'pro', 'sug', 'req', 'ans', 'arg', 'disarg', 'cor', 'acc', 'dec', 'bc', 'oth']):\n",
    "        if tag.startswith(t): return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "def get_word_id(word, oov, line, return_word=False):\n",
    "    ret = []\n",
    "    start = 0\n",
    "    end = len(word)\n",
    "    while start < len(word):\n",
    "        if start >= end: \n",
    "            if word in oov: oov[word] += 1\n",
    "            else: \n",
    "                oov[word] = 1\n",
    "                # print(line, word)\n",
    "            # return ret + [-1]\n",
    "            return ['<UNK>'] if return_word else [decoder_map_word['<UNK>']]\n",
    "        if word[start:end] in decoder_map_word:\n",
    "            if not return_word: ret.append(decoder_map_word[word[start:end]])\n",
    "            else: ret.append(word[start:end])\n",
    "            start = end\n",
    "            end = len(word)\n",
    "        else:\n",
    "            end -= 1\n",
    "    return ret\n",
    "\n",
    "def split_words(s, oov, return_word=False):\n",
    "    s = \" \".join(s)\n",
    "    s.replace(\"　\", \" \")\n",
    "    #s = s.replace(' <', '<')\n",
    "    #s = s.replace(' ', ' <sp> ')\n",
    "    #s = s.replace('<', ' <')\n",
    "    s = s.split(' ')\n",
    "    ret = []\n",
    "    for ss in s:\n",
    "        ss = ss.replace(' ', '')\n",
    "        if ss and ss[0] =='<':\n",
    "            ret.append(ss if return_word else decoder_map_word['<sp>'])\n",
    "            continue\n",
    "        if ss == '': continue\n",
    "        tokens = mt.parse(ss).strip()\n",
    "        tokens = tokens.split(' ')\n",
    "        for t in tokens:\n",
    "            ret += get_word_id(t, oov, s, return_word)\n",
    "    return ret\n",
    "    \n",
    "def preproc(line):\n",
    "    for c in ['I', 'L', 'F', 'D', 'N']:\n",
    "        line = re.sub(r'\\(' + c + '([^\\)]*)\\)', r'\\1', line)\n",
    "    line = re.sub(r'\\(\\?([^\\)]*)\\)', r'\\1', line)\n",
    "    for c in list('`\"「」') + ['(L', 'L)', '(F', ' L']:\n",
    "        line = line.replace(c, '')\n",
    "    line = re.sub(r'%.*', r'', line)\n",
    "        \n",
    "    line = line.replace('{LAUGH}', '')\n",
    "    line = line.replace('{LAuGH}', '')\n",
    "    line = line.replace('{COUGH}', '')\n",
    "    line = re.sub(r'\\(P [0-9]*\\)', r'', line)\n",
    "    \n",
    "    k = line.find('/')\n",
    "    if k != -1:\n",
    "        tags = [line[k + 1:].split(';')[0]]\n",
    "        line = line[:k]\n",
    "    else: tags = []\n",
    "        \n",
    "    env = None\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i].find('/') >= 0:\n",
    "            env, tags[i] = tags[i].split('/', maxsplit=1)\n",
    "    # tags = re.findall(r'\\/(\\w*)', line)\n",
    "    line = re.sub(r'\\/\\w*', '', line)\n",
    "    # if len(tags) > 0: line += \"<\" + tags[0] + \">\"\n",
    "    # if len(tags) > 1: print(tags)\n",
    "    # print(line)\n",
    "    return line.strip(), tags, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = []\n",
    "targets = []\n",
    "\n",
    "targets = []\n",
    "oov = {}\n",
    "dialogs = {}\n",
    "for DATA_FOLDER in TEST_DATA_FOLDERS if mode == \"test\" else TRAIN_DATA_FOLDERS:\n",
    "    for file in glob.glob(os.path.join(DATA_FOLDER, \"*.txt\")):\n",
    "        # print(file)\n",
    "        is_start = True\n",
    "        date, id, obj = os.path.basename(file)[:-4].split('_')[:3]\n",
    "        longid = date + \"_\" + id\n",
    "        year = date[:4]\n",
    "        \n",
    "        dialogs[longid] = []\n",
    "        # if longid != \"20171128_03\": continue\n",
    "        \n",
    "        encoding = 'shift-jis'\n",
    "        if longid in ['20161205_03', '20161209_05', '20161215_04', '20161208_03', '20161205_07', '20161212_01', '20161209_04', \n",
    "                      '20161209_03', '20161208_01', '20161212_03', '20161208_02', '20161207_05', '20161215_03', '20161207_04',\n",
    "                     '20161205_04']: encoding = 'utf-8'\n",
    "        \n",
    "        lines = open(file, encoding=encoding).read().split('\\n')\n",
    "        # print(lines)\n",
    "\n",
    "        wavpath = '/n/sd7/trung/csp/data/erica/dialogue/%s/%s/%s/%s_audio_mix.wav' % (year, date, longid, longid)\n",
    "        if not os.path.exists(os.path.join(OUTPUT_FOLDER, \"wav\", longid)):\n",
    "            os.mkdir(os.path.join(OUTPUT_FOLDER, \"wav\", longid))\n",
    "        if not os.path.exists(os.path.join(OUTPUT_FOLDER, \"htk\", longid)):\n",
    "            os.mkdir(os.path.join(OUTPUT_FOLDER, \"htk\", longid))\n",
    "        if not os.path.exists(os.path.join(OUTPUT_FOLDER, \"npy\", longid)):\n",
    "            os.mkdir(os.path.join(OUTPUT_FOLDER, \"npy\", longid))\n",
    "\n",
    "        k = 0\n",
    "        while k < len(lines):\n",
    "            line = lines[k].strip()\n",
    "            if longid == '20161209_05':\n",
    "                start, end = line.split(' ')[1:3]\n",
    "            else:\n",
    "                start, end = line.split(' ')[1].split('-')\n",
    "            start, end = float(start) * 1000, float(end) * 1000\n",
    "            utt_id = line.split(' ')[0]\n",
    "\n",
    "            output_wav = os.path.join(OUTPUT_FOLDER, \"wav\", longid, \"%s_%s.wav\" % (utt_id, obj))\n",
    "            output_htk = os.path.join(OUTPUT_FOLDER, \"htk\", longid, \"%s_%s.htk\" % (utt_id, obj))\n",
    "            output_npy = os.path.join(OUTPUT_FOLDER, \"npy\", longid, \"%s_%s.npy\" % (utt_id, obj))\n",
    "            \n",
    "            if line[-2:] == obj + ':':\n",
    "                k += 1\n",
    "                s = []\n",
    "                tags = []\n",
    "                envs = set()\n",
    "                while k < len(lines):\n",
    "                    line = lines[k].strip()\n",
    "                    if line[-2:] == obj + ':': break\n",
    "\n",
    "                    line, tag, env = preproc(line)\n",
    "                    if env: envs.add(env)\n",
    "                    if tag: tags += tag\n",
    "                    if line != '': s.append(line)\n",
    "                    # if len(tag) > 0: s.append(\"<\" + tag[0] + \">\")\n",
    "                    k += 1\n",
    "                \n",
    "                s = split_words(s, oov, False)\n",
    "                \n",
    "                tags = '_'.join([';'.join(tag) for tag in tags])\n",
    "                \n",
    "                if len(s) >= 2 and float(end) - float(start) < 10000:\n",
    "                    #targets.append(' '.join([str(k) for k in s]))\n",
    "                    dialogs[longid].append((start, end, obj, output_npy, ' '.join([str(k) for k in s]), is_start))\n",
    "                    is_start = False\n",
    "                    #targets.append(\"%s %07d %07d %s %s %s %s\" % (longid, start, end, utt_id, obj, ''.join(list(envs)) or '_', s))\n",
    "            else: k += 1\n",
    "        dialogs[longid].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs: 83\n",
      "243680\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogs:\", len(dialogs))\n",
    "keys = list(dialogs.keys()) * 20\n",
    "random.shuffle(keys)\n",
    "\n",
    "dlgs = [i for i in range(BATCH_SIZE)]\n",
    "dlg_pos = [0 for i in range(len(dlgs))]\n",
    "next_dlg = BATCH_SIZE\n",
    "\n",
    "fo = open(os.path.join(OUTPUT_FOLDER, \"train_group_by_dlg_16.txt\"), 'w')\n",
    "count = 0\n",
    "while next_dlg <= len(keys):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        dlg = dialogs[keys[dlgs[i]]]\n",
    "        if dlg_pos[i] >= len(dlg):\n",
    "            dlgs[i] = next_dlg\n",
    "            next_dlg += 1\n",
    "            dlg_pos[i] = 0\n",
    "\n",
    "        utt = dlg[dlg_pos[i]]\n",
    "        fo.write(\"%s\\t%d\\t%d\\t2 %s 1\\n\" % (utt[3], 1 if dlg_pos[i] == 0 else 0, 1 if utt[2] == 'U' else 0, utt[4]))\n",
    "        dlg_pos[i] += 1\n",
    "        count += 1\n",
    "        #print(count, \"%s\\t%d\\t%d\\t2 %s 1\\n\" % (utt[3], 1 if dlg_pos[i] == 0 else 0, 1 if utt[2] == 'U' else 0, utt[4]))\n",
    "            \n",
    "print(count)\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913\n"
     ]
    }
   ],
   "source": [
    "fo = open(os.path.join(OUTPUT_FOLDER, \"test_group_by_dlg_16.txt\"), 'w')\n",
    "count = 0\n",
    "for dlg in dialogs:\n",
    "    for utt in dlg:\n",
    "        fo.write(\"%s\\t%d\\t%d\\t2 %s 1\\n\" % (dlg[3], 1 if dlg_pos[i] == 0 else 0, 1 if dlg[2] == 'U' else 0, dlg[4]))\n",
    "        count += 1\n",
    "fo.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
