{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trungdv/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "DATA_FOLDER = \"/n/sd7/trung/csp/data/swbd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 11179\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"vocab\", \"word_freq5_300h.txt\")) as f:\n",
    "    words = f.read().split('\\n')\n",
    "    word_dict = {word: i for i, word in enumerate(words)}\n",
    "print(\"Vocab Size:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swda conversation: 1155\n"
     ]
    }
   ],
   "source": [
    "dlgs_swda = []\n",
    "for file in glob.glob(os.path.join(DATA_FOLDER, \"swda\", '**/*.csv')):\n",
    "    dlgid = os.path.basename(file).split('.')[0].split('_')[-1]\n",
    "    dlgs_swda.append(dlgid)\n",
    "\n",
    "print(\"swda conversation:\", len(dlgs_swda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full dataset: 2438\n"
     ]
    }
   ],
   "source": [
    "dlgs = []\n",
    "for file in glob.glob(os.path.join(DATA_FOLDER, \"swb_ms98_transcriptions\",  '**/**')):\n",
    "    dlgid = os.path.basename(file)\n",
    "    dlgs.append(dlgid)\n",
    "\n",
    "dlgs_test = []\n",
    "dlgs_train = dlgs\n",
    "#dlgs_test = dlgs[:20]\n",
    "#dlgs_train = dlgs[20:]\n",
    "print(\"full dataset:\", len(dlgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 278040\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"train\", \"word_freq5_start_frame.csv\")) as f:\n",
    "    lines = f.read().split('\\n')[1:]\n",
    "    lines = [line.split(',') for line in lines]\n",
    "    lines = [(line[2], line[3], line[4]) for line in lines if len(line) == 5]\n",
    "    \n",
    "print(\"Train Size:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size: 1782\n"
     ]
    }
   ],
   "source": [
    "#eval2000\n",
    "with open(os.path.join(DATA_FOLDER, \"test\", \"word_freq5_start_frame.csv\")) as f:\n",
    "    lines = f.read().split('\\n')[1:]\n",
    "    lines = [line.split(',') for line in lines if line != '']\n",
    "    lines_test = []\n",
    "    for line in lines:\n",
    "        s = line[4]\n",
    "        s = s.replace('do not', \"don't\").replace('it is', \"it's\").replace('cannot', \"can't\")\n",
    "        s = s.split('_')\n",
    "        s = list(filter(lambda x: x[0] != '%', s))\n",
    "        s = [word_dict[word] if word in word_dict else word_dict['OOV'] for word in s]\n",
    "        s = ' '.join([str(x) for x in s])\n",
    "        if len(s) > 0:\n",
    "            lines_test.append((line[2], line[3], s))\n",
    "    \n",
    "print(\"Test Size:\", len(lines_test))\n",
    "#print('\\n'.join([str(line[2]) for line in test_lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: eval2000\n",
    "dlgs_test = []\n",
    "dlgs_train = dlgs\n",
    "bucket_size = 3000\n",
    "\n",
    "mode = \"train\"\n",
    "if False:\n",
    "    _dlgs = dlgs_train if mode == \"train\" else dlgs_test\n",
    "    _lines = list(filter(lambda it: os.path.basename(it[0])[2:6] in dlgs, lines))\n",
    "    _lines = lines\n",
    "    #print(len(lines_train))\n",
    "    outputs = [\"%s\\t%s\" % (line[0], line[2]) for line in _lines]\n",
    "    #if mode == \"train\":\n",
    "    #    outputs.sort(key=lambda s: len(s))\n",
    "    \n",
    "    '''\n",
    "    writer = None\n",
    "    for i, line in enumerate(lines_train):\n",
    "        if mode == \"test\" and i == 0:\n",
    "            writer = tf.python_io.TFRecordWriter(os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s.tfrecords\" % (mode)))\n",
    "        if mode == \"train\" and i % bucket_size == 0:\n",
    "            if writer is not None: writer.close()\n",
    "            train_filename = os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s_%d.tfrecords\" % (mode, i // bucket_size + 1))\n",
    "            writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "            print(i, end=\" \")\n",
    "        inp = np.load(line[0])\n",
    "        inp = np.ndarray.flatten(inp)\n",
    "        feature = {\n",
    "            \"input\": tf.train.Feature(float_list=tf.train.FloatList(value=inp)),\n",
    "            \"target\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(x) for x in line[2].split(' ')]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "    '''\n",
    "    \n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_baseline_freq5_%s.txt\" % mode), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))\n",
    "        \n",
    "if True:\n",
    "    outputs = [\"%s\\t%s\" % (line[0], line[2]) for line in lines_test]\n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_baseline_freq5_%s.txt\" % \"test\"), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133117\n",
      "1980\n"
     ]
    }
   ],
   "source": [
    "# baseline no 2000\n",
    "dlgs_test = dlgs_swda[:15]\n",
    "dlgs_train = dlgs_swda[15:]\n",
    "mode = \"train\"\n",
    "for mode in [\"train\", \"test\"]:\n",
    "    _dlgs = dlgs_train if mode == \"train\" else dlgs_test\n",
    "    _lines = list(filter(lambda it: os.path.basename(it[0])[2:6] in _dlgs, lines))\n",
    "    #lines_train = lines\n",
    "    print(len(_lines))\n",
    "    outputs = [\"%s\\t%s\" % (line[0], line[2]) for line in _lines]\n",
    "    #if mode == \"train\":\n",
    "    #    outputs.sort(key=lambda s: len(s))\n",
    "    \n",
    "    '''\n",
    "    writer = None\n",
    "    for i, line in enumerate(lines_train):\n",
    "        if mode == \"test\" and i == 0:\n",
    "            writer = tf.python_io.TFRecordWriter(os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s.tfrecords\" % (mode)))\n",
    "        if mode == \"train\" and i % bucket_size == 0:\n",
    "            if writer is not None: writer.close()\n",
    "            train_filename = os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s_%d.tfrecords\" % (mode, i // bucket_size + 1))\n",
    "            writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "            print(i, end=\" \")\n",
    "        inp = np.load(line[0])\n",
    "        inp = np.ndarray.flatten(inp)\n",
    "        feature = {\n",
    "            \"input\": tf.train.Feature(float_list=tf.train.FloatList(value=inp)),\n",
    "            \"target\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(x) for x in line[2].split(' ')]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "    '''\n",
    "    \n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_baseline_freq5_inclusive_eval_da_%s.txt\" % mode), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278041\n"
     ]
    }
   ],
   "source": [
    "mode = \"train\"\n",
    "if True:\n",
    "    dlgs = dlgs_train if mode == \"train\" else dlgs_test\n",
    "    #lines_train = list(filter(lambda it: os.path.basename(it[0])[2:6] in dlgs, lines))\n",
    "    lines_train = lines\n",
    "    print(len(lines_train))\n",
    "    outputs = [\"%s\\t%s\\t%s\" % (line[0], line[1], line[2]) for line in lines_train]\n",
    "    #if mode == \"train\":\n",
    "    #    outputs.sort(key=lambda s: len(s))\n",
    "    \n",
    "    '''\n",
    "    writer = None\n",
    "    for i, line in enumerate(lines_train):\n",
    "        if mode == \"test\" and i == 0:\n",
    "            writer = tf.python_io.TFRecordWriter(os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s.tfrecords\" % (mode)))\n",
    "        if mode == \"train\" and i % bucket_size == 0:\n",
    "            if writer is not None: writer.close()\n",
    "            train_filename = os.path.join(DATA_FOLDER, mode, \"baseline_swda\", \"%s_%d.tfrecords\" % (mode, i // bucket_size + 1))\n",
    "            writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "            print(i, end=\" \")\n",
    "        inp = np.load(line[0])\n",
    "        inp = np.ndarray.flatten(inp)\n",
    "        feature = {\n",
    "            \"input\": tf.train.Feature(float_list=tf.train.FloatList(value=inp)),\n",
    "            \"target\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(x) for x in line[2].split(' ')]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "    '''\n",
    "    \n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_start_frame_freq5_%s.txt\" % mode), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))\n",
    "        \n",
    "if True:\n",
    "    outputs = [\"%s\\t%s\\t%s\" % (line[0], line[1], line[2]) for line in lines_test]\n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_start_frame_freq5_%s.txt\" % \"test\"), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132364\n",
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e96d57907cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         feature = {\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0m_ZIP_PREFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x03\\x04'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for mode in [\"train\", \"test\"]:\n",
    "    dlgs = dlgs_train if mode == \"train\" else dlgs_test\n",
    "    lines_train = list(filter(lambda it: os.path.basename(it[0])[2:6] in dlgs, lines))\n",
    "    print(len(lines_train))\n",
    "    outputs = [\"%s\\t%s\\t%s\" % (line[0], line[1], line[2]) for line in lines_train]\n",
    "    \n",
    "    \n",
    "    for i, line in enumerate(lines_train):\n",
    "        if i % bucket_size == 0:\n",
    "            train_filename = os.path.join(DATA_FOLDER, \"input_dlg_order_%s_%d.tfrecords\" % (mode, i // bucket_size + 1))\n",
    "            writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "            print(i, end=\" \")\n",
    "        inp = np.load(line[0])\n",
    "        inp = np.ndarray.flatten(inp)\n",
    "        feature = {\n",
    "            \"input\": tf.train.Feature(float_list=tf.train.FloatList(value=inp)),\n",
    "            \"target\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(x) for x in line[2].split(' ')]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    with open(os.path.join(DATA_FOLDER, \"inputs_dlg_order_%s.txt\" % mode), \"w\") as f:\n",
    "        f.write('\\n'.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
