{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swda import Transcript\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "DATA_FOLDER = '/n/sd7/trung/csp/data/swbd'\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load Dialogs: 100%|██████████| 1155/1155 [04:46<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialog Count: 1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dlgs = {}\n",
    "for file in tqdm(glob.glob(os.path.join(DATA_FOLDER, \"swda\", '**/*.csv')), desc=\"Load Dialogs\"):\n",
    "    trans = Transcript(file, os.path.join(DATA_FOLDER, \"swda\", 'swda-metadata.csv'))\n",
    "    dlgid = os.path.basename(file).split('_')[2].split('.')[0]\n",
    "    dlgs[dlgid] = list(trans.utterances)\n",
    "    \n",
    "print(\"Dialog Count:\", len(dlgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "dict_keys(['fo_o_fw_\"_by_bc', 'qw', 'h', 'sd', 'sv', 'b', 'x', '%', '+', 'qy', 'qrr', 'na', 'bk', 'ba', 'ny', '^q', 'aa', 'nn', 'fc', 'ad', 'qo', 'qh', 'no', 'ng', '^2', 'bh', 'qy^d', 'br', 'b^m', '^h', 'bf', 'fa', 'oo_co_cc', 'ar', 'bd', 't1', 'arp_nd', 't3', 'ft', '^g', 'qw^d', 'fp', 'aap_am'])\n"
     ]
    }
   ],
   "source": [
    "# process act tag\n",
    "da_tags = {}\n",
    "for dlgid in dlgs:\n",
    "    dlg = dlgs[dlgid]\n",
    "    for utt in dlg:\n",
    "        tag = utt.damsl_act_tag()\n",
    "        \n",
    "        if tag in da_tags: da_tags[tag] += 1\n",
    "        else: da_tags[tag] = 1\n",
    "    \n",
    "print(len(da_tags))\n",
    "da_tagids = { da: i for i, da in enumerate(list(da_tags.keys())) }\n",
    "print(da_tagids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build Vocab List: 100%|██████████| 1155/1155 [00:05<00:00, 204.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 21707\n",
      "['child_talking', \"fianc3ee's\", 'appliqu3ed', 'fianc3ee', 'fianc3e', 'blas3e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punctuations = ['?', '.', ',', ';', '!', ':']\n",
    "def split_words(sent):\n",
    "    if ' /' in sent: sent = sent[:sent.index(' /')]\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'\\{.', '', sent)\n",
    "    sent = re.sub(r'\\[\\[.*\\]\\]', '', sent)\n",
    "    sent = sent + ' '\n",
    "    for c in ['[', ']', '(', ')', '#', '}', '\"', '\\\\', '/', '*', '=', '+', '&']: sent = sent.replace(c, '')\n",
    "    for c in punctuations: sent = sent.replace(c, ' ' + c)\n",
    "    start = 0\n",
    "    i, br = 0, 0\n",
    "    words = []\n",
    "    while i < len(sent) and start < len(sent):\n",
    "        if sent[i] in ['[', '<', '{', '(']: br += 1\n",
    "        elif sent[i] in [']', '>', '}', ')']: br -= 1\n",
    "        elif sent[i] == ' ' and br == 0: \n",
    "            words.append(sent[start:i])\n",
    "            start = i + 1\n",
    "        i += 1\n",
    "    #words = word_tokenize(sent)\n",
    "    i = 0\n",
    "    ret = []\n",
    "    sqbr, acbr = 0, 0\n",
    "    for i, word in enumerate(words):\n",
    "        if word == '': continue\n",
    "        elif word[0] in ['<']: continue\n",
    "        elif word in ['typo']: continue\n",
    "        #elif not all('a' <= c <= 'z' or c in punctuations + ['-', \"'\"] for c in word): ret.append('<oov>')\n",
    "        #elif word in ['{', '}', '-', '(', ')', '#']: pass\n",
    "        #elif word == '[': sqbr += 1\n",
    "        #elif word == ']': sqbr -= 1\n",
    "        #elif word == '<': acbr += 1\n",
    "        #elif word == '>': acbr -= 1\n",
    "        #elif sqbr > 0 and word in ['+']: pass\n",
    "        #elif i > 0 and words[i - 1] == '{': pass\n",
    "        #elif acbr > 0: pass\n",
    "        #elif i > 0 and i < len(words) - 1 and words[i - 1] == '<' and words[i + 1] == '>': pass #ret.append('[%s]' % word.lower())\n",
    "        else: ret.append(word)\n",
    "    return ret\n",
    "\n",
    "vocab = set()\n",
    "for dlgid in tqdm(dlgs, desc=\"Build Vocab List\"):\n",
    "    for utt in dlgs[dlgid]:\n",
    "        for word in split_words(utt.text): vocab.add(word)\n",
    "\n",
    "word_ids = { word: i for i, word in enumerate(list(vocab)) }\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_raw.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(list(vocab)))\n",
    "\n",
    "print(\"Word Count:\", len(vocab))\n",
    "print([word for word in vocab if not all('a' <= c <= 'z' or c in punctuations + ['-', \"'\"] for c in word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_keys = list(dlgs.keys())\n",
    "dlgs_train = _keys[20:]\n",
    "dlgs_test = _keys[:20]\n",
    "\n",
    "dlgs_dev_set = dlgs_train[:40]\n",
    "dlgs_train_set = dlgs_train[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [00:29<00:00, 76.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126\n",
      "----- List of dialog with inconsistent caller id -----\n",
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trans_utts = {}\n",
    "tmp = set()\n",
    "\n",
    "Utt = namedtuple(\"Utt\", \"id, caller, start, end, act_tag, words, trans_words, npy\")\n",
    "\n",
    "for transfile in tqdm(list(glob.glob(os.path.join(DATA_FOLDER, \"ptree_transcripts\", \"alignments\", \"*.text\")))):\n",
    "    dlgid = os.path.basename(transfile)[2:6]\n",
    "    c = os.path.basename(transfile)[6]\n",
    "    \n",
    "    for t in [0]:\n",
    "        if not os.path.exists(os.path.join(DATA_FOLDER, \"wav\", dlgid)):\n",
    "            os.mkdir(os.path.join(DATA_FOLDER, \"wav\", dlgid))\n",
    "        #transfile = os.path.join(DATA_FOLDER, \"ptree_transcripts/alignments/sw%s%s-ms98-a-penn.text\" % (dlgid, c))\n",
    "        #if not os.path.exists(transfile): \n",
    "        #    print(\"(not existed: %s%s)\" % (dlgid, c), end=' ')\n",
    "        #    continue\n",
    "            \n",
    "        if dlgid not in trans_utts: trans_utts[dlgid] = {}\n",
    "            \n",
    "        with open(transfile) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = [line.split('\\t') for line in lines]\n",
    "            lines = [dict(\n",
    "                start=int(float(line[2]) * 100 + 0.05), #start\n",
    "                end=int(float(line[3]) * 100 + 0.05), #end\n",
    "                id=int(line[1].split('.')[-1]), #id\n",
    "                word=line[5].lower(), #word\n",
    "                caller=line[1].split('.')[0] #caller \n",
    "            ) for line in lines if len(line) == 7]\n",
    "            if lines[0]['caller'] != c: tmp.add(dlgid)\n",
    "            \n",
    "            cur = None\n",
    "            i = 0\n",
    "            ignored_ls = ['[silence]', '[noise]', '[laughter]', '[vocalized-noise]', '---', '+++', '<e_aside>', '<b_aside>', '-h', '-s']\n",
    "            while i < len(lines):\n",
    "                line = lines[i]\n",
    "                word = line['word']\n",
    "                for c in ['\"', \".\"]: word = word.replace(c, \"\")\n",
    "                if word in ignored_ls: pass\n",
    "                elif cur is not None and line['id'] == id:\n",
    "                    cur['words'].append(dict(start=line['start'], end=line['end'], word=word))\n",
    "                else:\n",
    "                    if cur is not None:\n",
    "                        trans_utts[dlgid][id] = cur\n",
    "                    cur = dict(words=[dict(start=line['start'], end=line['end'], word=word)], caller=c)\n",
    "                    id = line['id']\n",
    "                i += 1\n",
    "                \n",
    "            trans_utts[dlgid][id] = cur # (start, end, id, text)\n",
    "print(len(trans_utts))\n",
    "\n",
    "print(\"----- List of dialog with inconsistent caller id -----\")\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlgs = {dlgid: dlgs[dlgid] for dlgid in dlgs if dlgid in trans_utts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1126/1126 [00:11<00:00, 101.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Export acoustic features\n",
    "\n",
    "equi_pairs = [(\"that's\", \"that\"), (\"n't\", \"wouldn't\"), (\"it's\", \"it\"), (\"there\", \"there's\"),\n",
    "             (\"i\", \"i've\"), (\"you\", \"you're\"), (\"i\", \"i'm\"), (\"he\", \"he's\"), (\"not\", \"cannot\"), (\"A\", \"A's\"),\n",
    "             (\"m-'n\", \"'n\"), (\"twentys\", \"twenty's\"), (\"hinckleys\", \"hinckley's\"), (\"your\", \"you're\"), (\"it's\", \"its\"),\n",
    "             (\"the'vette\", \"'vette\"), (\"i'd\", \"'d\"), (\"watch'em\", \"'em\"), (\"brother's\", \"brothers\")]\n",
    "\n",
    "def preproc_pos_words(ls):\n",
    "    ret = []\n",
    "    for word in ls:\n",
    "        word = word.lower()\n",
    "        if word in ['']: continue\n",
    "        if ('a' > word[0] or word[0] > 'z') and word[0] not in list(\"'\"): continue\n",
    "        ret.append(word)\n",
    "    sent = ' '.join(ret)\n",
    "    rpl = [(\" n't\", \"n't\"), (\" '\", \"'\")]\n",
    "    for src, tgt in rpl: sent = sent.replace(src, tgt)\n",
    "    return sent.split(' ')\n",
    "    return ret\n",
    "\n",
    "def find_word(s, pos_words, pos):\n",
    "    word = pos_words[-1]\n",
    "    word = word.replace(\"''\", \"\")\n",
    "    if word == \"mumblex\": word = pos_words[-2]\n",
    "    for i in range(5):\n",
    "        for id in [pos - i, pos + i]:\n",
    "            if id < 0 or id > len(s) - 1: continue\n",
    "            w = s[id]['word'].lower()\n",
    "            w = ''.join([c for c in w if c not in list('\"')])\n",
    "            # print(w, word)\n",
    "            if w == word: return id\n",
    "            for s1, s2 in equi_pairs:\n",
    "                if w == s1 and word == s2 or w == s2 and word == s1:\n",
    "                    return id\n",
    "            if w.startswith(word) and len(word) * 2 > len(w): return id\n",
    "            if word == \"n't\" and w[-3:] == \"n't\": return id\n",
    "            if word == \"'d\" and w[-2:] == \"'d\": return id\n",
    "            if w[-2:] == \"'s\" and word[-1] == \"s\": return id\n",
    "            if w.startswith(word) and all(w[i] == '-' for i in range(len(word), len(w))): return id\n",
    "            if w.startswith(word) and w[-3:] in [\"'re\", \"'ve\", \"'ll\"]: return id\n",
    "\n",
    "dlg_utts = {}\n",
    "\n",
    "# Dialog with annotation\n",
    "for dlgid in tqdm(dlgs):\n",
    "    #if dlgid != '2495': continue\n",
    "    dlg = dlgs[dlgid]\n",
    "    dlg.sort(key=lambda utt: utt.transcript_index)\n",
    "    #print([utt.transcript_index for utt in dlg])\n",
    "    if dlgid not in trans_utts:\n",
    "        print(dlgid)\n",
    "        continue\n",
    "    dlg_utts[dlgid] = []\n",
    "    i = 0\n",
    "    while i < len(dlg): # loop through utterance in da\n",
    "        utt = dlg[i]\n",
    "        id = utt.utterance_index\n",
    "        if id not in trans_utts[dlgid]:\n",
    "            i += 1\n",
    "            continue\n",
    "        trans_utt = trans_utts[dlgid][id]['words']\n",
    "        trans_cur_pos = 0\n",
    "        while utt.utterance_index == id:\n",
    "            if dlgid == '3825' and utt.transcript_index == 5: \n",
    "                i += 1; utt = dlg[i]; continue\n",
    "            pos_words = preproc_pos_words(split_words(utt.text))\n",
    "            if len(pos_words) == 0: i += 1; break\n",
    "            if len(pos_words) == 1 and pos_words[0] in [\"\", \"mumblex\"]: i += 1; break\n",
    "            last_pos = find_word(trans_utt, pos_words, trans_cur_pos + len(pos_words) - 1)\n",
    "                \n",
    "            if last_pos is None:\n",
    "                print(dlgid)\n",
    "                print(utt.caller, utt.utterance_index, utt.transcript_index)\n",
    "                print('-->', [w['word'] for w in trans_utt])\n",
    "                print(trans_cur_pos + len(pos_words), \"/\", len(trans_utt))\n",
    "                \n",
    "                # print(dlg[i - 2].transcript_index, dlg[i - 2].act_tag, dlg[i - 2].pos_words())\n",
    "                print(dlg[i - 1].transcript_index, dlg[i - 1].act_tag, preproc_pos_words(dlg[i - 1].pos_words()))\n",
    "                print(dlg[i].transcript_index, dlg[i].act_tag, pos_words)\n",
    "                if i + 1 < len(dlg):\n",
    "                    print(dlg[i + 1].transcript_index, dlg[i + 1].act_tag, preproc_pos_words(dlg[i + 1].pos_words()))\n",
    "                print(id, [w['word'] for w in trans_utt[trans_cur_pos:last_pos + 1]])\n",
    "            \n",
    "            dlg_utts[dlgid].append(dict(\n",
    "                id=utt.transcript_index,\n",
    "                utt_id=utt.utterance_index,\n",
    "                caller=utt.caller,\n",
    "                start=trans_utt[trans_cur_pos]['start'],#start=trans_utts[dlgid][cur_id]['start'], \n",
    "                end=trans_utt[last_pos]['end'],#end=trans_utts[dlgid][cur_id]['end'],\n",
    "                act_tag=utt.damsl_act_tag(),\n",
    "                words=pos_words,\n",
    "                pos_from=trans_cur_pos,\n",
    "                pos_to=last_pos,\n",
    "                trans_words=[w['word'] for w in trans_utt[trans_cur_pos:last_pos + 1]],\n",
    "                #npy=os.path.join(DATA_FOLDER, \"features\", \"npy\", dlgid, \"%s_%s.npy\" % (cur_id, caller))\n",
    "            ))\n",
    "            \n",
    "            if abs(len(pos_words) - (last_pos - trans_cur_pos + 1)) > 2:\n",
    "                print('-->', [w['word'] for w in trans_utt])\n",
    "                print(\"%s\\n%s\" % (dlg_utts[dlgid][-3]['words'], [w for w in dlg_utts[dlgid][-3]['trans_words']]))\n",
    "                print(\"%s\\n%s\" % (dlg_utts[dlgid][-2]['words'], [w for w in dlg_utts[dlgid][-2]['trans_words']]))\n",
    "                print(\"%s %d\\n%s\" % (dlg_utts[dlgid][-1]['words'], dlg_utts[dlgid][-1]['pos_from'], [w for w in dlg_utts[dlgid][-1]['trans_words']]))\n",
    "                print(trans_cur_pos)\n",
    "            \n",
    "            trans_cur_pos = last_pos + 1\n",
    "            i += 1\n",
    "            if i < len(dlg): utt = dlg[i]\n",
    "            else: break\n",
    "        #print(da_utts[longid].keys())\n",
    "    #da_utts[dlgid][cur_id] = cur\n",
    "\n",
    "    #dlg_utts[dlgid].sort(key=lambda utt: utt.id)\n",
    "\n",
    "# Dialog without annotation\n",
    "\n",
    "print(len(dlg_utts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1126/1126 [00:03<00:00, 372.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dlg_utts_seg = {}\n",
    "for id in tqdm(dlg_utts.keys()):\n",
    "    ls = []\n",
    "    cur = None\n",
    "    for utt in dlg_utts[id]:\n",
    "        if cur is not None and cur['caller'] == utt['caller']:\n",
    "            cur['end'] = utt['end']\n",
    "            cur['act_tag'].append(utt['act_tag'])\n",
    "            cur['trans_words'] += utt['trans_words'] + ['</da_%s>' % utt['act_tag']]\n",
    "        else:\n",
    "            if cur is not None: ls.append(cur)\n",
    "            cur = copy.deepcopy(utt)\n",
    "            cur['trans_words'].append('</da_%s>' % utt['act_tag'])\n",
    "            #cur['trans_words'].append('</da>')\n",
    "            cur['act_tag'] = [cur['act_tag']]\n",
    "        #if cur is not None: print(cur['trans_words'])\n",
    "    dlg_utts_seg[id] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1126/1126 [23:27<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from htk import read as read_htk\n",
    "sil_duration = 25\n",
    "PREFIX = \"swda_seg_padding25_speaker_norm\"\n",
    "\n",
    "total_frame_num = 0\n",
    "reverse_caller = [\"3061\", \"2064\", \"2854\", \"2968\", \"2960\", \"2794\", \"2954\", \"2543\", \"3077\"] # these dialogs have incorrect caller annotation\n",
    "for dlgid in tqdm(dlg_utts_seg):\n",
    "    #if dlgid not in reverse_caller: continue\n",
    "    #print(dlgid)\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid))\n",
    "    for caller in ['A', 'B']:\n",
    "        utterance_dict = list(filter(lambda utt: utt['caller'] == caller, dlg_utts_seg[dlgid]))\n",
    "        audio_path = os.path.join(DATA_FOLDER, \"htk\", \"swbd\", \"sw0%s-%s.htk\" % (dlgid, caller if dlgid not in reverse_caller else (\"A\" if caller == \"B\" else \"B\")))\n",
    "        \n",
    "        input_data, _, _ = read_htk(audio_path)\n",
    "        feature_dim = input_data.shape[1]\n",
    "        input_data_dict = {}\n",
    "        total_frame_num = 0\n",
    "        end_frame_pre = 0\n",
    "        global_mean = None\n",
    "        global_std = None\n",
    "        input_data_utt_std = np.zeros((feature_dim,), dtype=np.float32)\n",
    "        input_data_utt_sum = np.zeros((feature_dim,), dtype=np.float32)\n",
    "        \n",
    "        for k in range(3):\n",
    "            for i, utt in enumerate(utterance_dict):\n",
    "                start_frame, end_frame = utt['start'], utt['end']\n",
    "                if i == 0:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, 0)\n",
    "                    start_frame_next = utterance_dict[i + 1]['start']\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "                    end_frame_pre = end_frame\n",
    "                elif i == len(utterance_dict) - 1:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, input_data.shape[0]))\n",
    "                else:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                    start_frame_next = utterance_dict[i + 1]['start']\n",
    "                    if end_frame > start_frame_next:\n",
    "                        print(\"Warning: utterances are overlapping.\")\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "                    end_frame_pre = end_frame\n",
    "            \n",
    "                #print(end_frame_extend - start_frame_extend, end=\" \")\n",
    "                #start_frame_extend, end_frame_extend = start_frame, end_frame\n",
    "                #print(end_frame_extend - start_frame_extend, end_frame - start_frame)\n",
    "                # if k == 0: print(start_frame_extend, start_frame, end_frame, end_frame_extend)\n",
    "                input_data_utt = input_data[start_frame_extend:end_frame_extend]\n",
    "                input_data_utt_sum += np.sum(input_data_utt, axis=0)\n",
    "        \n",
    "                if global_mean is not None:\n",
    "                    if global_std is None:\n",
    "                        input_data_utt_std += np.sum(np.abs(input_data_utt - global_mean) ** 2, axis=0)\n",
    "                    else: # save\n",
    "                        input_utt = (input_data_utt - global_mean) / global_std\n",
    "                        #print(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid, \"%s%s.npy\" % (utt['id'], caller)))\n",
    "                        np.save(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid, \"%s%s.npy\" % (utt['id'], caller)), input_utt)\n",
    "                #total_frame_num_file += end_frame_extend - start_frame_extend\n",
    "                #input_data_dict[utt['id']] = input_data_utt\n",
    "                total_frame_num += end_frame_extend - start_frame_extend\n",
    "                \n",
    "            if global_mean is not None:\n",
    "                if global_std is None:\n",
    "                    global_std = np.sqrt(input_data_utt_std / (total_frame_num - 1))\n",
    "                    #np.save(INPUT_STD_PATH, global_std)\n",
    "                    #print(total_frame_num, \"global_std\", global_std)\n",
    "            else:\n",
    "                global_mean = input_data_utt_sum / total_frame_num\n",
    "                #np.save(INPUT_MEAN_PATH, global_mean)\n",
    "                #print(total_frame_num, \"global_mean\", global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1095/1095 [00:03<00:00, 354.11it/s]\n",
      "test: 100%|██████████| 20/20 [00:00<00:00, 243.63it/s]\n",
      "dev: 100%|██████████| 40/40 [00:00<00:00, 268.75it/s]\n"
     ]
    }
   ],
   "source": [
    "words_full = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_full_old.txt\")).read().split('\\n')\n",
    "words_full = {word: i for i, word in enumerate(words_full)}\n",
    "# build test set for speech recognition result\n",
    "#words_full['</da>'] = len(words_full) + 2\n",
    "l = len(words_full)\n",
    "for da in da_tagids.keys(): words_full['</da_%s>' % da] = l + da_tagids[da]\n",
    "with open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_full_old_da.txt\"), 'w') as f:\n",
    "    f.write('\\n'.join(words_full))\n",
    "    \n",
    "# for speech recogniton\n",
    "PREFIX = \"swda_seg_da_full_vocab_old\"\n",
    "#words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda.txt\")).read().split('\\n')\n",
    "#words = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "dlgs_dev_set = dlgs_train[:40]\n",
    "dlgs_train_set = dlgs_train[40:]\n",
    "#dlgs_dev_set = ['2053', '2067', '2071', '2072', '2160', '2163', '2175', '2253', '2289', '2299', '2340', '2373', '2395', '2399', '2455', '2501', '2534', '2558', '2593', '2594', '2598', '2620', '2621', '2623', '2630', '2653', '2713', '2755', '2772', '2776', '2790', '2832', '2839', '2842', '2854', '2874', '2888', '2889', '2944', '2959', '2981', '2989', '3015', '3046', '3072', '3096', '3148', '3156', '3181', '3184', '3190', '3191', '3202', '3207', '3239', '3246', '3250', '3251', '3255', '3257', '3281', '3288', '3290', '3291', '3334', '3346', '3352', '3354', '3382', '3433', '3445', '3491', '3497', '3500', '3506', '3509', '3554', '3576', '3584', '3587', '3658', '3659', '3666', '3675', '3686', '3697', '3711', '3769', '3797', '3810', '3811', '3921', '4004', '4026', '4037', '4048', '4072', '4318', '4321', '4347', '4356', '4372', '4572', '4633', '4660', '4697', '4707', '4716', '4736', '4802', '4890', '4917']\n",
    "#dlgs_train_set = [k for k in dlgs_train if k not in dlgs_dev_set]\n",
    "headers = ['dialog_id', 'sound', 'start', 'end', 'sound_len', 'caller', 'dialog_act', 'text', 'target', 'predicted_text']\n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(DATA_FOLDER, '%s_split20_%s.csv' % (PREFIX, mode)), 'w') as fo:\n",
    "        fo.write('\\t'.join(headers) + '\\n')\n",
    "        for dlgid in tqdm(dlgs_test if mode == \"test\" else (dlgs_train_set if mode == \"train\" else dlgs_dev_set), desc=mode):\n",
    "            #if dlgid != '3061': continue\n",
    "            #print(len(dlg_utts[dlgid]))\n",
    "            if dlgid not in dlg_utts: continue\n",
    "            for utt in dlg_utts_seg[dlgid]:\n",
    "                if len(utt['trans_words']) == 0: print(dlgid)\n",
    "                if utt['start'] >= utt['end'] - 5: continue\n",
    "                fo.write('\\t'.join([\n",
    "                    dlgid,\n",
    "                    os.path.join(DATA_FOLDER, \"feature\", \"numpy\", \"swda_seg_padding25_speaker_norm\", dlgid, \"%d%s.npy\" % (utt['id'], utt['caller'])), \n",
    "                    str(utt['start']), str(utt['end']),\n",
    "                    str(utt['end'] - utt['start']),\n",
    "                    utt['caller'],\n",
    "                    #utt['id'], \n",
    "                    str(','.join([str(da_tagids[tag]) for tag in utt['act_tag']])),\n",
    "                    ' '.join([word.lower().replace('-', '') for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']])\n",
    "                ]) + '\\n')\n",
    "#print(([word for word in oov if '-' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27328\n"
     ]
    }
   ],
   "source": [
    "print(len(words_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1126/1126 [00:03<00:00, 314.70it/s]\n"
     ]
    }
   ],
   "source": [
    "dlg_utts_seg = {}\n",
    "for id in tqdm(dlg_utts.keys()):\n",
    "    ls = []\n",
    "    cur = None\n",
    "    for utt in dlg_utts[id]:\n",
    "        if cur is not None and cur['caller'] == utt['caller']:\n",
    "            cur['end'] = utt['end']\n",
    "            cur['act_tag'].append(utt['act_tag'])\n",
    "            cur['trans_words'] += utt['trans_words']\n",
    "            cur['da_tag_seq'] += [43] * (len(utt['trans_words']) - 1) + [da_tagids[utt['act_tag']]]\n",
    "        else:\n",
    "            if cur is not None: ls.append(cur)\n",
    "            cur = copy.deepcopy(utt)\n",
    "            cur['da_tag_seq'] = [43] * (len(utt['trans_words']) - 1) + [da_tagids[utt['act_tag']]]\n",
    "            #cur['trans_words'].append('</da>')\n",
    "            cur['act_tag'] = [cur['act_tag']]\n",
    "        #if cur is not None: print(cur['trans_words'])\n",
    "    dlg_utts_seg[id] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1095/1095 [00:03<00:00, 350.08it/s]\n",
      "test: 100%|██████████| 20/20 [00:00<00:00, 265.20it/s]\n",
      "dev: 100%|██████████| 40/40 [00:00<00:00, 266.14it/s]\n"
     ]
    }
   ],
   "source": [
    "words_full = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_full_old.txt\")).read().split('\\n')\n",
    "words_full = {word: i for i, word in enumerate(words_full)}\n",
    "# build test set for speech recognition result\n",
    "\n",
    "# for speech recogniton\n",
    "PREFIX = \"swda_seg_da_seq_full_vocab_old\"\n",
    "#words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda.txt\")).read().split('\\n')\n",
    "#words = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "dlgs_dev_set = dlgs_train[:40]\n",
    "dlgs_train_set = dlgs_train[40:]\n",
    "#dlgs_dev_set = ['2053', '2067', '2071', '2072', '2160', '2163', '2175', '2253', '2289', '2299', '2340', '2373', '2395', '2399', '2455', '2501', '2534', '2558', '2593', '2594', '2598', '2620', '2621', '2623', '2630', '2653', '2713', '2755', '2772', '2776', '2790', '2832', '2839', '2842', '2854', '2874', '2888', '2889', '2944', '2959', '2981', '2989', '3015', '3046', '3072', '3096', '3148', '3156', '3181', '3184', '3190', '3191', '3202', '3207', '3239', '3246', '3250', '3251', '3255', '3257', '3281', '3288', '3290', '3291', '3334', '3346', '3352', '3354', '3382', '3433', '3445', '3491', '3497', '3500', '3506', '3509', '3554', '3576', '3584', '3587', '3658', '3659', '3666', '3675', '3686', '3697', '3711', '3769', '3797', '3810', '3811', '3921', '4004', '4026', '4037', '4048', '4072', '4318', '4321', '4347', '4356', '4372', '4572', '4633', '4660', '4697', '4707', '4716', '4736', '4802', '4890', '4917']\n",
    "#dlgs_train_set = [k for k in dlgs_train if k not in dlgs_dev_set]\n",
    "headers = ['dialog_id', 'sound', 'start', 'end', 'sound_len', 'caller', 'dialog_act', 'text', 'target', 'predicted_text']\n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(DATA_FOLDER, '%s_split20_%s.csv' % (PREFIX, mode)), 'w') as fo:\n",
    "        fo.write('\\t'.join(headers) + '\\n')\n",
    "        for dlgid in tqdm(dlgs_test if mode == \"test\" else (dlgs_train_set if mode == \"train\" else dlgs_dev_set), desc=mode):\n",
    "            #if dlgid != '3061': continue\n",
    "            #print(len(dlg_utts[dlgid]))\n",
    "            if dlgid not in dlg_utts: continue\n",
    "            for utt in dlg_utts_seg[dlgid]:\n",
    "                assert len(utt['da_tag_seq']) == len(utt['trans_words'])\n",
    "                if len(utt['trans_words']) == 0: print(dlgid)\n",
    "                if utt['start'] >= utt['end'] - 5: continue\n",
    "                fo.write('\\t'.join([\n",
    "                    dlgid,\n",
    "                    os.path.join(DATA_FOLDER, \"feature\", \"numpy\", \"swda_seg_padding25_speaker_norm\", dlgid, \"%d%s.npy\" % (utt['id'], utt['caller'])), \n",
    "                    str(utt['start']), str(utt['end']),\n",
    "                    str(utt['end'] - utt['start']),\n",
    "                    utt['caller'],\n",
    "                    #utt['id'], \n",
    "                    str(','.join([str(tag) for tag in utt['da_tag_seq']])),\n",
    "                    ' '.join([word.lower().replace('-', '') for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']])\n",
    "                ]) + '\\n')\n",
    "#print(([word for word in oov if '-' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1095/1095 [00:03<00:00, 346.61it/s]\n",
      "test: 100%|██████████| 20/20 [00:00<00:00, 261.52it/s]\n",
      "dev: 100%|██████████| 40/40 [00:00<00:00, 256.20it/s]\n"
     ]
    }
   ],
   "source": [
    "words_full = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_full_old.txt\")).read().split('\\n')\n",
    "words_full = {word: i for i, word in enumerate(words_full)}\n",
    "# build test set for speech recognition result\n",
    "\n",
    "# for speech recogniton\n",
    "PREFIX = \"swda_seg_only_da_seq_full_vocab_old\"\n",
    "#words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda.txt\")).read().split('\\n')\n",
    "#words = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "dlgs_dev_set = dlgs_train[:40]\n",
    "dlgs_train_set = dlgs_train[40:]\n",
    "#dlgs_dev_set = ['2053', '2067', '2071', '2072', '2160', '2163', '2175', '2253', '2289', '2299', '2340', '2373', '2395', '2399', '2455', '2501', '2534', '2558', '2593', '2594', '2598', '2620', '2621', '2623', '2630', '2653', '2713', '2755', '2772', '2776', '2790', '2832', '2839', '2842', '2854', '2874', '2888', '2889', '2944', '2959', '2981', '2989', '3015', '3046', '3072', '3096', '3148', '3156', '3181', '3184', '3190', '3191', '3202', '3207', '3239', '3246', '3250', '3251', '3255', '3257', '3281', '3288', '3290', '3291', '3334', '3346', '3352', '3354', '3382', '3433', '3445', '3491', '3497', '3500', '3506', '3509', '3554', '3576', '3584', '3587', '3658', '3659', '3666', '3675', '3686', '3697', '3711', '3769', '3797', '3810', '3811', '3921', '4004', '4026', '4037', '4048', '4072', '4318', '4321', '4347', '4356', '4372', '4572', '4633', '4660', '4697', '4707', '4716', '4736', '4802', '4890', '4917']\n",
    "#dlgs_train_set = [k for k in dlgs_train if k not in dlgs_dev_set]\n",
    "headers = ['dialog_id', 'sound', 'start', 'end', 'sound_len', 'caller', 'dialog_act', 'text', 'target', 'predicted_text']\n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(DATA_FOLDER, '%s_split20_%s.csv' % (PREFIX, mode)), 'w') as fo:\n",
    "        fo.write('\\t'.join(headers) + '\\n')\n",
    "        for dlgid in tqdm(dlgs_test if mode == \"test\" else (dlgs_train_set if mode == \"train\" else dlgs_dev_set), desc=mode):\n",
    "            #if dlgid != '3061': continue\n",
    "            #print(len(dlg_utts[dlgid]))\n",
    "            if dlgid not in dlg_utts: continue\n",
    "            for utt in dlg_utts_seg[dlgid]:\n",
    "                assert len(utt['da_tag_seq']) == len(utt['trans_words'])\n",
    "                if len(utt['trans_words']) == 0: print(dlgid)\n",
    "                if utt['start'] >= utt['end'] - 5: continue\n",
    "                fo.write('\\t'.join([\n",
    "                    dlgid,\n",
    "                    os.path.join(DATA_FOLDER, \"feature\", \"numpy\", \"swda_seg_padding25_speaker_norm\", dlgid, \"%d%s.npy\" % (utt['id'], utt['caller'])), \n",
    "                    str(utt['start']), str(utt['end']),\n",
    "                    str(utt['end'] - utt['start']),\n",
    "                    utt['caller'],\n",
    "                    #utt['id'], \n",
    "                    str(','.join([str(1 if tag == 44 else 2) for tag in utt['da_tag_seq']])),\n",
    "                    ' '.join([word.lower().replace('-', '') for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words_full[word.lower().replace('-', '')]) if word.lower().replace('-', '') in words_full else '0' for word in utt['trans_words']])\n",
    "                ]) + '\\n')\n",
    "#print(([word for word in oov if '-' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1095/1095 [00:12<00:00, 89.92it/s]\n",
      "test: 100%|██████████| 20/20 [00:00<00:00, 46.19it/s]\n",
      "dev: 100%|██████████| 40/40 [00:00<00:00, 92.62it/s]\n"
     ]
    }
   ],
   "source": [
    "words_full = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_full_old.txt\")).read().split('\\n')\n",
    "words_full = {word: i for i, word in enumerate(words_full)}\n",
    "# build test set for speech recognition result\n",
    "\n",
    "# for speech recogniton\n",
    "PREFIX = \"swda_seg_da_seq_insert_bound_full_vocab_old\"\n",
    "#words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda.txt\")).read().split('\\n')\n",
    "#words = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "dlgs_dev_set = dlgs_train[:40]\n",
    "dlgs_train_set = dlgs_train[40:]\n",
    "#dlgs_dev_set = ['2053', '2067', '2071', '2072', '2160', '2163', '2175', '2253', '2289', '2299', '2340', '2373', '2395', '2399', '2455', '2501', '2534', '2558', '2593', '2594', '2598', '2620', '2621', '2623', '2630', '2653', '2713', '2755', '2772', '2776', '2790', '2832', '2839', '2842', '2854', '2874', '2888', '2889', '2944', '2959', '2981', '2989', '3015', '3046', '3072', '3096', '3148', '3156', '3181', '3184', '3190', '3191', '3202', '3207', '3239', '3246', '3250', '3251', '3255', '3257', '3281', '3288', '3290', '3291', '3334', '3346', '3352', '3354', '3382', '3433', '3445', '3491', '3497', '3500', '3506', '3509', '3554', '3576', '3584', '3587', '3658', '3659', '3666', '3675', '3686', '3697', '3711', '3769', '3797', '3810', '3811', '3921', '4004', '4026', '4037', '4048', '4072', '4318', '4321', '4347', '4356', '4372', '4572', '4633', '4660', '4697', '4707', '4716', '4736', '4802', '4890', '4917']\n",
    "#dlgs_train_set = [k for k in dlgs_train if k not in dlgs_dev_set]\n",
    "headers = ['dialog_id', 'sound', 'start', 'end', 'sound_len', 'caller', 'dialog_act', 'text', 'target', 'predicted_text']\n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(DATA_FOLDER, '%s_split20_%s.csv' % (PREFIX, mode)), 'w') as fo:\n",
    "        fo.write('\\t'.join(headers) + '\\n')\n",
    "        for dlgid in tqdm(dlgs_test if mode == \"test\" else (dlgs_train_set if mode == \"train\" else dlgs_dev_set), desc=mode):\n",
    "            #if dlgid != '3061': continue\n",
    "            #print(len(dlg_utts[dlgid]))\n",
    "            if dlgid not in dlg_utts: continue\n",
    "            for utt in dlg_utts_seg[dlgid]:\n",
    "                assert len(utt['da_tag_seq']) == len(utt['trans_words'])\n",
    "                if len(utt['trans_words']) == 0: print(dlgid)\n",
    "                if utt['start'] >= utt['end'] - 5: continue\n",
    "                \n",
    "                trans = []\n",
    "                for i, word in enumerate(utt['trans_words']):\n",
    "                    word = word.lower().replace('-', '')\n",
    "                    trans.append(str(words_full[word]) if word in words_full else '0')\n",
    "                    if utt['da_tag_seq'][i] != 44: trans.append('27287')\n",
    "                fo.write('\\t'.join([\n",
    "                    dlgid,\n",
    "                    os.path.join(DATA_FOLDER, \"feature\", \"numpy\", \"swda_seg_padding25_speaker_norm\", dlgid, \"%d%s.npy\" % (utt['id'], utt['caller'])), \n",
    "                    str(utt['start']), str(utt['end']),\n",
    "                    str(utt['end'] - utt['start']),\n",
    "                    utt['caller'],\n",
    "                    #utt['id'], \n",
    "                    str(','.join([str(da_tagids[tag]) for tag in utt['act_tag']])),\n",
    "                    #str(','.join([str(tag) for tag in utt['da_tag_seq']])),\n",
    "                    ' '.join([word.lower().replace('-', '') for word in utt['trans_words']]),\n",
    "                    ' '.join(trans),\n",
    "                    ' '.join(trans)\n",
    "                ]) + '\\n')\n",
    "#print(([word for word in oov if '-' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
