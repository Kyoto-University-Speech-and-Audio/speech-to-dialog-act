{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from swda import Transcript\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "DATA_FOLDER = '/n/sd7/trung/csp/data/swbd'\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from swda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load Dialogs: 100%|██████████| 1155/1155 [04:53<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialog Count: 1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dlgs = {}\n",
    "for file in tqdm(glob.glob(os.path.join(DATA_FOLDER, \"swda\", '**/*.csv')), desc=\"Load Dialogs\"):\n",
    "    trans = Transcript(file, os.path.join(DATA_FOLDER, \"swda\", 'swda-metadata.csv'))\n",
    "    dlgid = os.path.basename(file).split('_')[2].split('.')[0]\n",
    "    dlgs[dlgid] = list(trans.utterances)\n",
    "    \n",
    "print(\"Dialog Count:\", len(dlgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "{'fo_o_fw_\"_by_bc': 0, 'qw': 1, 'h': 2, 'sd': 3, 'sv': 4, 'b': 5, 'x': 6, '%': 7, '+': 8, 'qy': 9, 'qrr': 10, 'na': 11, 'bk': 12, 'ba': 13, 'ny': 14, '^q': 15, 'aa': 16, 'nn': 17, 'fc': 18, 'ad': 19, 'qo': 20, 'qh': 21, 'no': 22, 'ng': 23, '^2': 24, 'bh': 25, 'qy^d': 26, 'br': 27, 'b^m': 28, '^h': 29, 'bf': 30, 'fa': 31, 'oo_co_cc': 32, 'ar': 33, 'bd': 34, 't1': 35, 'arp_nd': 36, 't3': 37, 'ft': 38, '^g': 39, 'qw^d': 40, 'fp': 41, 'aap_am': 42}\n"
     ]
    }
   ],
   "source": [
    "# process act tag\n",
    "da_tags = {}\n",
    "for dlgid in dlgs:\n",
    "    dlg = dlgs[dlgid]\n",
    "    for utt in dlg:\n",
    "        tag = utt.damsl_act_tag()\n",
    "        \n",
    "        if tag in da_tags: da_tags[tag] += 1\n",
    "        else: da_tags[tag] = 1\n",
    "    \n",
    "print(len(da_tags))\n",
    "da_tagids = { da: i for i, da in enumerate(list(da_tags.keys())) }\n",
    "print(da_tagids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_keys = list(dlgs.keys())\n",
    "dlgs_train = _keys[20:]\n",
    "dlgs_test = _keys[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build Vocab List: 100%|██████████| 1155/1155 [00:02<00:00, 443.60it/s]\n",
      "Build Dataset (train): 100%|██████████| 1095/1095 [00:10<00:00, 100.25it/s]\n",
      "Build Dataset (test): 100%|██████████| 20/20 [00:00<00:00, 92.52it/s]\n",
      "Build Dataset (dev): 100%|██████████| 40/40 [00:00<00:00, 90.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 20887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for dlgid in tqdm(dlgs, desc=\"Build Vocab List\"):\n",
    "    for utt in dlgs[dlgid]:\n",
    "        for word in utt.pos_words(): vocab.add(word.lower())\n",
    "\n",
    "word_ids = { word: i for i, word in enumerate(list(vocab)) }\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda_raw.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(list(vocab)))\n",
    "    \n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    _dlgs = dlgs_test if mode == \"test\" else (dlgs_train[40:] if mode == \"train\" else dlgs_train[:40])\n",
    "    with open(os.path.join(DATA_FOLDER, \"swda_raw_%s.txt\" % mode), \"w\") as f:\n",
    "        f.write('\\t'.join([\"dialog_id\", \"sound\", \"caller\", \"dialog_act\", \"text\", \"predicted_text\", \"target\"]) + '\\n')\n",
    "        for dlgid in tqdm(_dlgs, desc=\"Build Dataset (%s)\" % mode):\n",
    "            for utt in dlgs[dlgid]:\n",
    "                if len(utt.pos_words()) == 0: continue\n",
    "                f.write('\\t'.join([\n",
    "                    dlgid, \n",
    "                    \"none\",\n",
    "                    utt.caller, \n",
    "                    str(da_tagids[utt.damsl_act_tag()]), \n",
    "                    ' '.join(utt.pos_words()), \n",
    "                    ' '.join([str(word_ids[word.lower()]) for word in utt.pos_words()]),\n",
    "                    ' '.join([str(word_ids[word.lower()]) for word in utt.pos_words()])\n",
    "                ]))\n",
    "                f.write('\\n')\n",
    "\n",
    "print(\"Word Count:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transcript (ms98 Penn Treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [00:09<00:00, 237.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trans_utts = {}\n",
    "\n",
    "Utt = namedtuple(\"Utt\", \"id, caller, start, end, act_tag, words, trans_words, npy\")\n",
    "\n",
    "for transfile in tqdm(list(glob.glob(os.path.join(DATA_FOLDER, \"ptree_transcripts\", \"alignments\", \"*.text\")))):\n",
    "    dlgid = os.path.basename(transfile)[2:6]\n",
    "    c = os.path.basename(transfile)[6]\n",
    "    \n",
    "    for t in [0]:\n",
    "        if not os.path.exists(os.path.join(DATA_FOLDER, \"wav\", dlgid)):\n",
    "            os.mkdir(os.path.join(DATA_FOLDER, \"wav\", dlgid))\n",
    "        #transfile = os.path.join(DATA_FOLDER, \"ptree_transcripts/alignments/sw%s%s-ms98-a-penn.text\" % (dlgid, c))\n",
    "        #if not os.path.exists(transfile): \n",
    "        #    print(\"(not existed: %s%s)\" % (dlgid, c), end=' ')\n",
    "        #    continue\n",
    "            \n",
    "        if dlgid not in trans_utts: trans_utts[dlgid] = {}\n",
    "            \n",
    "        with open(transfile) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = [line.split('\\t') for line in lines]\n",
    "            lines = [dict(\n",
    "                start=int(float(line[2]) * 100 + 0.05), #start\n",
    "                end=int(float(line[3]) * 100 + 0.05), #end\n",
    "                id=int(line[1].split('.')[-1]), #id\n",
    "                word=line[5].lower(), #word\n",
    "                caller=line[1].split('.')[0] #caller \n",
    "            ) for line in lines if len(line) == 7]\n",
    "            \n",
    "            cur = None\n",
    "            i = 0\n",
    "            ignored_ls = ['[silence]', '[noise]', '[laughter]', '[vocalized-noise]', '---', '+++', '<e_aside>', '<b_aside>', '-h', '-s']\n",
    "            while i < len(lines):\n",
    "                line = lines[i]\n",
    "                if line['word'] in ignored_ls: pass\n",
    "                elif cur is not None and line['id'] == id:\n",
    "                    cur['words'].append(dict(start=line['start'], end=line['end'], word=line['word']))\n",
    "                else:\n",
    "                    if cur is not None:\n",
    "                        trans_utts[dlgid][id] = cur\n",
    "                    cur = dict(words=[dict(start=line['start'], end=line['end'], word=line['word'])], caller=c)\n",
    "                    id = line['id']\n",
    "                i += 1\n",
    "                \n",
    "            trans_utts[dlgid][id] = cur # (start, end, id, text)\n",
    "print(len(trans_utts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align transcript with swda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1155/1155 [00:07<00:00, 149.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Export acoustic features\n",
    "\n",
    "equi_pairs = [(\"that's\", \"that\"), (\"n't\", \"wouldn't\"), (\"it's\", \"it\"), (\"there\", \"there's\"),\n",
    "             (\"i\", \"i've\"), (\"you\", \"you're\"), (\"i\", \"i'm\"), (\"he\", \"he's\"), (\"not\", \"cannot\"), (\"A\", \"A's\"),\n",
    "             (\"m-'n\", \"'n\"), (\"twentys\", \"twenty's\"), (\"hinckleys\", \"hinckley's\"), (\"your\", \"you're\"), (\"it's\", \"its\"),\n",
    "             (\"the'vette\", \"'vette\"), (\"i'd\", \"'d\"), (\"watch'em\", \"'em\"), (\"brother's\", \"brothers\")]\n",
    "\n",
    "def preproc_pos_words(ls):\n",
    "    ret = []\n",
    "    for word in ls:\n",
    "        word = word.lower()\n",
    "        if word in ['']: continue\n",
    "        if ('a' > word[0] or word[0] > 'z') and word[0] not in list(\"'\"): continue\n",
    "        ret.append(word)\n",
    "    sent = ' '.join(ret)\n",
    "    rpl = [(\" n't\", \"n't\"), (\" '\", \"'\")]\n",
    "    for src, tgt in rpl: sent = sent.replace(src, tgt)\n",
    "    return sent.split(' ')\n",
    "    return ret\n",
    "\n",
    "def find_word(s, pos_words, pos):\n",
    "    word = pos_words[-1]\n",
    "    word = word.replace(\"''\", \"\")\n",
    "    if word == \"mumblex\": word = pos_words[-2]\n",
    "    for i in range(5):\n",
    "        for id in [pos - i, pos + i]:\n",
    "            if id < 0 or id > len(s) - 1: continue\n",
    "            w = s[id]['word'].lower()\n",
    "            w = ''.join([c for c in w if c not in list('\"')])\n",
    "            # print(w, word)\n",
    "            if w == word: return id\n",
    "            for s1, s2 in equi_pairs:\n",
    "                if w == s1 and word == s2 or w == s2 and word == s1:\n",
    "                    return id\n",
    "            if w.startswith(word) and len(word) * 2 > len(w): return id\n",
    "            if word == \"n't\" and w[-3:] == \"n't\": return id\n",
    "            if word == \"'d\" and w[-2:] == \"'d\": return id\n",
    "            if w[-2:] == \"'s\" and word[-1] == \"s\": return id\n",
    "            if w.startswith(word) and all(w[i] == '-' for i in range(len(word), len(w))): return id\n",
    "            if w.startswith(word) and w[-3:] in [\"'re\", \"'ve\", \"'ll\"]: return id\n",
    "\n",
    "dlg_utts = {}\n",
    "\n",
    "# Dialog with annotation\n",
    "for dlgid in tqdm(dlgs):\n",
    "    dlg = dlgs[dlgid]\n",
    "    dlg.sort(key=lambda utt: utt.transcript_index)\n",
    "    #print([utt.transcript_index for utt in dlg])\n",
    "    if dlgid not in trans_utts: continue\n",
    "    dlg_utts[dlgid] = []\n",
    "    i = 0\n",
    "    while i < len(dlg): # loop through utterance in da\n",
    "        utt = dlg[i]\n",
    "        id = utt.utterance_index\n",
    "        if id not in trans_utts[dlgid]:\n",
    "            i += 1\n",
    "            continue\n",
    "        trans_utt = trans_utts[dlgid][id]['words']\n",
    "        trans_cur_pos = 0\n",
    "        while utt.utterance_index == id:\n",
    "            pos_words = preproc_pos_words(utt.pos_words())\n",
    "            if len(pos_words) == 0: i += 1; break\n",
    "            if len(pos_words) == 1 and pos_words[0] in [\"\", \"mumblex\"]: i += 1; break\n",
    "            last_pos = find_word(trans_utt, pos_words, trans_cur_pos + len(pos_words) - 1)\n",
    "            if last_pos is None:\n",
    "                print(utt.caller, utt.utterance_index, utt.transcript_index)\n",
    "                print('-->', [w['word'] for w in trans_utt])\n",
    "                print(trans_cur_pos + len(pos_words), \"/\", len(trans_utt))\n",
    "                # print(dlg[i - 2].transcript_index, dlg[i - 2].act_tag, dlg[i - 2].pos_words())\n",
    "                print(dlg[i - 1].transcript_index, dlg[i - 1].act_tag, preproc_pos_words(dlg[i - 1].pos_words()))\n",
    "                print(dlg[i].transcript_index, dlg[i].act_tag, pos_words)\n",
    "                if i + 1 < len(dlg):\n",
    "                    print(dlg[i + 1].transcript_index, dlg[i + 1].act_tag, preproc_pos_words(dlg[i + 1].pos_words()))\n",
    "                print(id, [w['word'] for w in trans_utt[trans_cur_pos:last_pos + 1]])\n",
    "            \n",
    "            dlg_utts[dlgid].append(dict(\n",
    "                id=utt.transcript_index,\n",
    "                utt_id=utt.utterance_index,\n",
    "                caller=utt.caller,\n",
    "                start=trans_utt[trans_cur_pos]['start'],#start=trans_utts[dlgid][cur_id]['start'], \n",
    "                end=trans_utt[last_pos]['end'],#end=trans_utts[dlgid][cur_id]['end'],\n",
    "                act_tag=utt.damsl_act_tag(),\n",
    "                words=pos_words,\n",
    "                pos_from=trans_cur_pos,\n",
    "                pos_to=last_pos,\n",
    "                trans_words=[w['word'] for w in trans_utt[trans_cur_pos:last_pos + 1]],\n",
    "                #npy=os.path.join(DATA_FOLDER, \"features\", \"npy\", dlgid, \"%s_%s.npy\" % (cur_id, caller))\n",
    "            ))\n",
    "            \n",
    "            if abs(len(pos_words) - (last_pos - trans_cur_pos + 1)) > 2:\n",
    "                print('-->', [w['word'] for w in trans_utt])\n",
    "                print(\"%s\\n%s\" % (dlg_utts[dlgid][-3]['words'], [w for w in dlg_utts[dlgid][-3]['trans_words']]))\n",
    "                print(\"%s\\n%s\" % (dlg_utts[dlgid][-2]['words'], [w for w in dlg_utts[dlgid][-2]['trans_words']]))\n",
    "                print(\"%s %d\\n%s\" % (dlg_utts[dlgid][-1]['words'], dlg_utts[dlgid][-1]['pos_from'], [w for w in dlg_utts[dlgid][-1]['trans_words']]))\n",
    "                print(trans_cur_pos)\n",
    "            \n",
    "            trans_cur_pos = last_pos + 1\n",
    "            i += 1\n",
    "            if i < len(dlg): utt = dlg[i]\n",
    "            else: break\n",
    "        #print(da_utts[longid].keys())\n",
    "    #da_utts[dlgid][cur_id] = cur\n",
    "\n",
    "    #dlg_utts[dlgid].sort(key=lambda utt: utt.id)\n",
    "\n",
    "# Dialog without annotation\n",
    "\n",
    "print(len(dlg_utts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREFIX = \"swda\"\n",
    "PREFIX = \"swda_padding15\"\n",
    "INPUT_MEAN_PATH = os.path.join(DATA_FOLDER, \"%s_mean.npy\" % PREFIX)\n",
    "INPUT_STD_PATH = os.path.join(DATA_FOLDER, \"%s_std.npy\" % PREFIX)\n",
    "\n",
    "global_mean = np.load(INPUT_MEAN_PATH) if os.path.exists(INPUT_MEAN_PATH) else None\n",
    "global_std = np.load(INPUT_STD_PATH) if os.path.exists(INPUT_STD_PATH) else None\n",
    "normalize = 'speaker'\n",
    "global_mean = None\n",
    "global_std = None\n",
    "print(global_mean, global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htk import read as read_htk\n",
    "sil_duration = 15\n",
    "\n",
    "total_frame_num = 0\n",
    "input_data_utt_sum = np.zeros((feature_dim,), dtype=np.float32)\n",
    "for dlgid in tqdm(dlg_utts):\n",
    "    #print(dlgid)\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid))\n",
    "    for caller in ['A', 'B']:\n",
    "        utterance_dict = list(filter(lambda utt: utt['caller'] == caller, dlg_utts[dlgid]))\n",
    "        audio_path = os.path.join(DATA_FOLDER, \"htk\", \"swbd\", \"sw0%s-%s.htk\" % (dlgid, caller))\n",
    "        input_data, _, _ = read_htk(audio_path)\n",
    "        feature_dim = input_data.shape[1]\n",
    "        input_data_dict = {}\n",
    "        total_frame_num_file = 0\n",
    "        end_frame_pre = 0\n",
    "        mean = None\n",
    "        input_data_utt_std = np.zeros((feature_dim,), dtype=np.float32)\n",
    "        \n",
    "        for i, utt in enumerate(utterance_dict):\n",
    "            start_frame, end_frame = utt['start'], utt['end']\n",
    "            if i == 0:\n",
    "                start_frame_extend = max(start_frame - sil_duration, 0)\n",
    "                start_frame_next = utterance_dict[i + 1]['start']\n",
    "                end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "            elif i == len(utterance_dict) - 1:\n",
    "                start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                end_frame_extend = max(end_frame, min(end_frame + sil_duration, input_data.shape[0]))\n",
    "            else:\n",
    "                start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                start_frame_next = utterance_dict[i + 1]['start']\n",
    "                if end_frame > start_frame_next:\n",
    "                    print(\"Warning: utterances are overlapping.\")\n",
    "                end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "                end_frame_pre = end_frame\n",
    "            \n",
    "            #print(end_frame_extend - start_frame_extend, end=\" \")\n",
    "            #start_frame_extend, end_frame_extend = start_frame, end_frame\n",
    "            #print(end_frame_extend - start_frame_extend, end_frame - start_frame)\n",
    "            input_data_utt = input_data[start_frame_extend:end_frame_extend]\n",
    "            input_data_utt_sum += np.sum(input_data_utt, axis=0)\n",
    "            if global_mean is not None:\n",
    "                if global_std is None:\n",
    "                    input_data_utt_std += np.sum(np.abs(input_data_utt - global_mean) ** 2, axis=0)\n",
    "                else: # save\n",
    "                    input_utt = (input_data_utt - global_mean) / global_std\n",
    "                    np.save(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid, \"%s%s.npy\" % (utt['id'], caller)), input_utt)\n",
    "            #total_frame_num_file += end_frame_extend - start_frame_extend\n",
    "            input_data_dict[utt['id']] = input_data_utt\n",
    "            total_frame_num += end_frame_extend - start_frame_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_mean is not None:\n",
    "    if global_std is None:\n",
    "        global_std = np.sqrt(input_data_utt_std / (total_frame_num - 1))\n",
    "        np.save(INPUT_STD_PATH, global_std)\n",
    "        print(\"global_std\", global_std)\n",
    "else:\n",
    "    global_mean = input_data_utt_sum / total_frame_num\n",
    "    np.save(INPUT_MEAN_PATH, global_mean)\n",
    "    print(\"global_mean\", global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1135/1135 [00:00<00:00, 1400.83it/s]\n",
      "  3%|▎         | 34/1095 [00:00<00:03, 339.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 21314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:02<00:00, 418.76it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 328.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 328.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# for speech recogniton\n",
    "PREFIX = \"swda_padding25_speaker_norm\"\n",
    "words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words_swda.txt\")).read().split('\\n')\n",
    "words = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "if True: # build words\n",
    "    vocab_freq = {}\n",
    "    for dlgid in tqdm(dlgs_train):\n",
    "        if dlgid not in dlg_utts: continue\n",
    "        for utt in dlg_utts[dlgid]:\n",
    "            for word in utt['trans_words']:\n",
    "                word = word.lower()\n",
    "                if word == '': continue\n",
    "                if not 'a' <= word[0] <= 'z': continue\n",
    "                if word in vocab_freq: vocab_freq[word] += 1\n",
    "                else: vocab_freq[word] = 1\n",
    "\n",
    "    words = list(vocab_freq.keys())\n",
    "    words.sort(key=lambda word: vocab_freq[word], reverse=True)\n",
    "    words = words[:-1]\n",
    "    words = [\"<oov>\"] + words\n",
    "    print(\"Vocab Size:\", len(words))\n",
    "    with open(os.path.join(DATA_FOLDER, \"vocab\", \"words_20.txt\"), 'w') as f:\n",
    "        f.write('\\n'.join(words))\n",
    "    words = { word: i for i, word in enumerate(words) }\n",
    "\n",
    "headers = ['dialog_id', 'sound', 'sound_len', 'caller', 'dialog_act', 'text', 'target', 'predicted_text']\n",
    "for mode in [\"train\", \"test\", \"dev\"]:\n",
    "    with open(os.path.join(DATA_FOLDER, 'inputs_%s_split20_%s.txt' % (PREFIX, mode)), 'w') as fo:\n",
    "        fo.write('\\t'.join(headers) + '\\n')\n",
    "        for dlgid in tqdm(dlgs_test if mode == \"test\" else (dlgs_train[40:] if mode == \"train\" else dlgs_train[:40])):\n",
    "            if dlgid not in dlg_utts: continue\n",
    "            for utt in dlg_utts[dlgid]:\n",
    "                if len(utt['trans_words']) == 0: continue\n",
    "                if utt['start'] >= utt['end'] - 5: continue\n",
    "                fo.write('%s\\t%s\\t%d\\t%s\\t%s\\t%s\\t%s\\t%s\\n' % \n",
    "                    (dlgid,\n",
    "                    os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid, \"%d%s.npy\" % (utt['id'], utt['caller'])), \n",
    "                    utt['end'] - utt['start'],\n",
    "                    utt['caller'],\n",
    "                    #utt['id'], \n",
    "                    #utt['end'] - utt['start'],\n",
    "                    da_tagids[utt['act_tag']],\n",
    "                    ' '.join([word.lower() for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words[word.lower()]) if word.lower() in words else '0' for word in utt['trans_words']]),\n",
    "                    ' '.join([str(words[word.lower()]) if word.lower() in words else '0' for word in utt['trans_words']]),\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speaker padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htk import read as read_htk\n",
    "sil_duration = 25\n",
    "PREFIX = \"swda_padding25_speaker_norm\"\n",
    "\n",
    "total_frame_num = 0\n",
    "for dlgid in tqdm(dlg_utts):\n",
    "    #print(dlgid)\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid))\n",
    "    for caller in ['A', 'B']:\n",
    "        utterance_dict = list(filter(lambda utt: utt['caller'] == caller, dlg_utts[dlgid]))\n",
    "        audio_path = os.path.join(DATA_FOLDER, \"htk\", \"swbd\", \"sw0%s-%s.htk\" % (dlgid, caller))\n",
    "        input_data, _, _ = read_htk(audio_path)\n",
    "        feature_dim = input_data.shape[1]\n",
    "        input_data_dict = {}\n",
    "        total_frame_num = 0\n",
    "        end_frame_pre = 0\n",
    "        global_mean = None\n",
    "        global_std = None\n",
    "        input_data_utt_std = np.zeros((feature_dim,), dtype=np.float32)\n",
    "        input_data_utt_sum = np.zeros((feature_dim,), dtype=np.float32)\n",
    "        \n",
    "        for k in range(3):\n",
    "            for i, utt in enumerate(utterance_dict):\n",
    "                start_frame, end_frame = utt['start'], utt['end']\n",
    "                if i == 0:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, 0)\n",
    "                    start_frame_next = utterance_dict[i + 1]['start']\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "                    end_frame_pre = end_frame\n",
    "                elif i == len(utterance_dict) - 1:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, input_data.shape[0]))\n",
    "                else:\n",
    "                    start_frame_extend = max(start_frame - sil_duration, (start_frame + end_frame_pre) // 2)\n",
    "                    start_frame_next = utterance_dict[i + 1]['start']\n",
    "                    if end_frame > start_frame_next:\n",
    "                        print(\"Warning: utterances are overlapping.\")\n",
    "                    end_frame_extend = max(end_frame, min(end_frame + sil_duration, (start_frame_next + end_frame) // 2))\n",
    "                    end_frame_pre = end_frame\n",
    "            \n",
    "                #print(end_frame_extend - start_frame_extend, end=\" \")\n",
    "                #start_frame_extend, end_frame_extend = start_frame, end_frame\n",
    "                #print(end_frame_extend - start_frame_extend, end_frame - start_frame)\n",
    "                input_data_utt = input_data[start_frame_extend:end_frame_extend]\n",
    "                input_data_utt_sum += np.sum(input_data_utt, axis=0)\n",
    "        \n",
    "                if global_mean is not None:\n",
    "                    if global_std is None:\n",
    "                        input_data_utt_std += np.sum(np.abs(input_data_utt - global_mean) ** 2, axis=0)\n",
    "                    else: # save\n",
    "                        input_utt = (input_data_utt - global_mean) / global_std\n",
    "                        np.save(os.path.join(DATA_FOLDER, \"feature\", \"numpy\", PREFIX, dlgid, \"%s%s.npy\" % (utt['id'], caller)), input_utt)\n",
    "                #total_frame_num_file += end_frame_extend - start_frame_extend\n",
    "                #input_data_dict[utt['id']] = input_data_utt\n",
    "                total_frame_num += end_frame_extend - start_frame_extend\n",
    "                \n",
    "            if global_mean is not None:\n",
    "                if global_std is None:\n",
    "                    global_std = np.sqrt(input_data_utt_std / (total_frame_num - 1))\n",
    "                    #np.save(INPUT_STD_PATH, global_std)\n",
    "                    #print(total_frame_num, \"global_std\", global_std)\n",
    "            else:\n",
    "                global_mean = input_data_utt_sum / total_frame_num\n",
    "                #np.save(INPUT_MEAN_PATH, global_mean)\n",
    "                #print(total_frame_num, \"global_mean\", global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversations: 1126\n",
      "Utterances: 214592\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'end'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-448a9e9ebd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conversations:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances' Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-448a9e9ebd00>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conversations:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances' Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-448a9e9ebd00>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conversations:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utterances' Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length: %.2f hours\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdlg_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlg_utts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'end'"
     ]
    }
   ],
   "source": [
    "#dlg_utts = { id: dlg_utts[id] for id in dlg_utts if len(dlg_utts[id]) > 0 }\n",
    "print(\"Conversations:\", len(dlg_utts))\n",
    "print(\"Utterances:\", sum([len(dlg_utts[id]) for id in dlg_utts]))\n",
    "print(\"Utterances' Length: %.2f hours\" % (sum([sum([utt.end - utt.start for utt in dlg_utts[id]]) for id in dlg_utts]) / 3600))\n",
    "print(\"Length: %.2f hours\" % (sum([dlg_utts[id][-1].end for id in dlg_utts]) / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18783\n",
      "18783\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "mode = \"train\"\n",
    "dlg_keys = dlgs_test if mode == \"test\" else dlgs_train\n",
    "for longid in dlg_keys:\n",
    "    if longid not in dlg_utts: break\n",
    "    utts = dlg_utts[longid]\n",
    "    for utt in utts:\n",
    "        for word in utt['trans_words']:\n",
    "            word = word.lower()\n",
    "            if word in vocab: vocab[word] += 1\n",
    "            else: vocab[word] = 1\n",
    "                \n",
    "# print(len([word for word in vocab if vocab[word] == 1]))\n",
    "print(len(vocab))\n",
    "vocab2 = [word for word in vocab if vocab[word] >= 1]\n",
    "print(len(vocab2))\n",
    "vocab2.sort()\n",
    "with open(os.path.join(DATA_FOLDER, \"vocab\", \"words20.txt\"), 'w') as f:\n",
    "    #f.write('<unk>\\n')\n",
    "    f.write('\\n'.join(['%s' % (word) for i, word in enumerate(vocab2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "from struct import unpack, pack\n",
    "\n",
    "#mean = np.load(os.path.join(DATA_FOLDER, \"mean.npy\"))\n",
    "#var = np.load(os.path.join(DATA_FOLDER, \"var.npy\"))\n",
    "\n",
    "outputs = []\n",
    "mean = np.array([0] * 120)\n",
    "var = np.array([0] * 120)\n",
    "count = 0\n",
    "\n",
    "for longid in dlg_keys:\n",
    "    if longid != '2955': continue\n",
    "    print(longid, end=' ')\n",
    "    wav = {}\n",
    "    for c in ['A', 'B']:\n",
    "        wavpath = os.path.join(DATA_FOLDER, \"wav/sw0%s_%s.wav\" % (longid, c))\n",
    "        wav[c] = AudioSegment.from_wav(wavpath)\n",
    "    utts = dlg_utts[longid]\n",
    "    \n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"features\", \"wav\", longid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"features\", \"wav\", longid))\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"features\", \"npy\", longid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"features\", \"npy\", longid))\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"features\", \"htk\", longid)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, \"features\", \"htk\", longid))\n",
    "    # count += 1\n",
    "    \n",
    "    for utt in utts:\n",
    "        id = utt.id\n",
    "        c = utt.caller\n",
    "        output_wav = os.path.join(DATA_FOLDER, \"features\", \"wav\", longid, \"%s_%s.wav\" % (id, c))\n",
    "        output_npy = os.path.join(DATA_FOLDER, \"features\", \"npy\", longid, \"%s_%s.npy\" % (id, c))\n",
    "        output_htk = os.path.join(DATA_FOLDER, \"features\", \"htk\", longid, \"%s_%s.htk\" % (id, c))\n",
    "            \n",
    "        if True: #\n",
    "            # utt_wav = AudioSegment.silent(500) + wav[c][int(utt.start * 1000):int(utt.end * 1000)] + AudioSegment.silent(500)\n",
    "            utt_wav = wav[c][int(utt.start * 1000):int(utt.end * 1000)]\n",
    "            utt_wav.export(output_wav, format='wav', bitrate=16000)\n",
    "\n",
    "            call([\n",
    "                \"/n/sd7/trung/bin/htk/HTKTools/HCopy\",\n",
    "                output_wav,\n",
    "                output_htk,\n",
    "                \"-C\", \"/n/sd7/trung/config.lmfb.40ch\"\n",
    "            ])\n",
    "        \n",
    "        fh = open(output_htk, \"rb\")\n",
    "        spam = fh.read(12)\n",
    "        nSamples, sampPeriod, sampSize, parmKind = unpack(\">IIHH\", spam)\n",
    "        veclen = int(sampSize / 4)\n",
    "        fh.seek(12, 0)\n",
    "            \n",
    "        dat = np.fromfile(fh, dtype=np.float32)\n",
    "        dat = dat.reshape(len(dat) // veclen, veclen)\n",
    "        dat = dat.byteswap()\n",
    "        # print(utt.start - utt.end, len(dat))\n",
    "        \n",
    "        for k in range(len(dat)):\n",
    "            count += 1\n",
    "            updated_mean = mean + (dat[k] - mean) / count\n",
    "            var = var + ((dat[k] - mean) * (dat[k] - updated_mean) - var) / count\n",
    "            mean = updated_mean\n",
    "        \n",
    "        \n",
    "        #dat = (dat - mean) / np.sqrt(var)\n",
    "        # print(output_wav, len(dat))\n",
    "                \n",
    "        fh.close()\n",
    "        np.save(output_npy, dat)\n",
    "    \n",
    "np.save(os.path.join(DATA_FOLDER, \"mean.npy\"), mean)\n",
    "np.save(os.path.join(DATA_FOLDER, \"var.npy\"), var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b1aea3f2ad4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             outputs.append(\"%s %s\" % (\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mutt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'npy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             ))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'npy'"
     ]
    }
   ],
   "source": [
    "words = open(os.path.join(DATA_FOLDER, \"vocab\", \"words20.txt\")).read().split('\\n')\n",
    "words = {word: i for i, word in enumerate(words)}\n",
    "outputs = []\n",
    "for longid in dlg_keys:\n",
    "    utts = dlg_utts[longid]\n",
    "    for utt in utts:\n",
    "        if len(utt['trans_words']) > 0 and utt['end'] - utt['start'] < 15:\n",
    "            outputs.append(\"%s %s\" % (\n",
    "                utt['npy'],\n",
    "                ' '.join(['2'] + [words[w.lower()] if w.lower() in words else str(words['<unk>']) for w in utt['trans_words']] + ['1'])\n",
    "            ))\n",
    "\n",
    "outputs.sort(key=lambda o: len(o))\n",
    "print(len(outputs))\n",
    "with open(os.path.join(DATA_FOLDER, 'inputs_%s.txt' % (mode)), 'w') as fo:\n",
    "    fo.write('\\n'.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython, random\n",
    "#npy_path = \"/n/sd7/trung/csp/data/swb/features/npy/2955/1_B.npy\"\n",
    "npy_path = random.choice(dlg_utts[random.choice(list(dlg_utts.keys()))]).npy\n",
    "print(npy_path)\n",
    "longid = npy_path.split('/')[-2]\n",
    "uttid = npy_path.split('/')[-1].split('.')[0]\n",
    "utt = [utt for utt in dlg_utts[longid] if utt.id == int(uttid.split('_')[0])][0]\n",
    "print(utt.caller, utt.id, utt.trans_words)\n",
    "print(' '.join(['2'] + [words[w.lower()] if w.lower() in words else '0' for w in utt.trans_words] + ['1']))\n",
    "dat = np.load(npy_path)\n",
    "print(len(dat))\n",
    "print(np.sum(dat, axis=0))\n",
    "IPython.display.Audio(npy_path.replace('npy', 'wav'), autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(filename=\"/n/sd7/trung/csp/data/swb/wav/sw0%s_%s.wav\" % ('2955', 'B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob.glob(os.path.join(DATA_FOLDER, \"features\", '**/*.npy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
