{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "TEST_DATA_FOLDERS = [\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/interview\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/dating\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/test/attentive\"\n",
    "]\n",
    "TRAIN_DATA_FOLDERS = [\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/interview\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/dating\",\n",
    "    \"/n/sd7/trung/csp/data/erica/annotation/kyoto17/spring/attentive\",\n",
    "    #\"/n/sd7/trung/csp/data/erica/annotation/kyoto16/interview\",\n",
    "    #\"/n/sd7/trung/csp/data/erica/annotation/kyoto16/dating\",\n",
    "    #\"/n/sd7/trung/csp/data/erica/annotation/kyoto16/attentive\",\n",
    "    #\"/n/sd7/trung/csp/data/erica/annotation/kyoto16/labintro\",\n",
    "]\n",
    "\n",
    "mode = \"train\"\n",
    "\n",
    "from subprocess import call\n",
    "from struct import unpack, pack\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "\n",
    "import MeCab\n",
    "mt = MeCab.Tagger(\"-Owakati\")\n",
    "mt.parse('')\n",
    "\n",
    "OUTPUT_FOLDER = \"/n/sd7/trung/csp/data/erica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30638\n"
     ]
    }
   ],
   "source": [
    "words = [s.strip().split(' ', 1) for s in open('/n/rd32/mimura/e2e/data/script/aps_sps/word.id', encoding='eucjp')]\n",
    "decoder_map_word = {word[0].split('+')[0]: int(word[1]) for word in words}\n",
    "\n",
    "# words = [s.strip().split(' ', 1) for s in open('/n/sd7/trung/csp/data/erica/word_ids.txt') if s != \"\"]\n",
    "# decoder_map_word = {word[0]: int(word[1]) for word in words}\n",
    "# decoder_map_word = {'<unk>': 0, '<sos>': 1, '<eos>': 2, '<sp>': 3}\n",
    "print(\"Vocab Size:\", len(decoder_map_word))\n",
    "\n",
    "TAG_COUNT = 17\n",
    "def get_tag_id(tag):\n",
    "    for i, t in enumerate(['pQ', 'cQ', 'sQ', 'checkQ', 'inf', 'off', 'pro', 'sug', 'req', 'ans', 'arg', 'disarg', 'cor', 'acc', 'dec', 'bc', 'oth']):\n",
    "        if tag.startswith(t): return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "def get_word_id(word, oov, line, return_word=False):\n",
    "    '''\n",
    "    if word == '　': return ['<sp>']\n",
    "    if mode == \"train\" or word in decoder_map_word: \n",
    "        if word in vocab: vocab[word] += 1\n",
    "        else: \n",
    "            vocab[word] = 1\n",
    "            return [word]\n",
    "    else:\n",
    "        if word in oov: oov[word] += 1\n",
    "        else: oov[word] = 1\n",
    "        return [word]\n",
    "    '''\n",
    "    \n",
    "    ret = []\n",
    "    start = 0\n",
    "    end = len(word)\n",
    "    while start < len(word):\n",
    "        if start >= end: \n",
    "            if word in oov: oov[word] += 1\n",
    "            else: \n",
    "                oov[word] = 1\n",
    "                # print(line, word)\n",
    "            # return ret + [-1]\n",
    "            return ['<UNK>'] if return_word else [decoder_map_word['<UNK>']]\n",
    "        if word[start:end] in decoder_map_word:\n",
    "            if not return_word: ret.append(decoder_map_word[word[start:end]])\n",
    "            else: ret.append(word[start:end])\n",
    "            start = end\n",
    "            end = len(word)\n",
    "        else:\n",
    "            end -= 1\n",
    "    return ret\n",
    "\n",
    "def split_words(s, oov, return_word=False):\n",
    "    s = \" \".join(s)\n",
    "    s.replace(\"　\", \" \")\n",
    "    #s = s.replace(' <', '<')\n",
    "    #s = s.replace(' ', ' <sp> ')\n",
    "    #s = s.replace('<', ' <')\n",
    "    s = s.split(' ')\n",
    "    ret = []\n",
    "    for ss in s:\n",
    "        ss = ss.replace(' ', '')\n",
    "        if ss and ss[0] =='<':\n",
    "            ret.append(ss if return_word else decoder_map_word['<sp>'])\n",
    "            continue\n",
    "        if ss == '': continue\n",
    "        tokens = mt.parse(ss).strip()\n",
    "        tokens = tokens.split(' ')\n",
    "        for t in tokens:\n",
    "            ret += get_word_id(t, oov, s, return_word)\n",
    "    return ret\n",
    "    \n",
    "def preproc(line):\n",
    "    for c in ['I', 'L', 'F', 'D', 'N']:\n",
    "        line = re.sub(r'\\(' + c + '([^\\)]*)\\)', r'\\1', line)\n",
    "    line = re.sub(r'\\(\\?([^\\)]*)\\)', r'\\1', line)\n",
    "    for c in list('`\"「」') + ['(L', 'L)', '(F', ' L']:\n",
    "        line = line.replace(c, '')\n",
    "    line = re.sub(r'%.*', r'', line)\n",
    "        \n",
    "    line = line.replace('{LAUGH}', '')\n",
    "    line = line.replace('{LAuGH}', '')\n",
    "    line = line.replace('{COUGH}', '')\n",
    "    line = re.sub(r'\\(P [0-9]*\\)', r'', line)\n",
    "    \n",
    "    k = line.find('/')\n",
    "    if k != -1:\n",
    "        tags = [line[k + 1:].split(';')[0]]\n",
    "        line = line[:k]\n",
    "    else: tags = []\n",
    "        \n",
    "    env = None\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i].find('/') >= 0:\n",
    "            env, tags[i] = tags[i].split('/', maxsplit=1)\n",
    "    # tags = re.findall(r'\\/(\\w*)', line)\n",
    "    line = re.sub(r'\\/\\w*', '', line)\n",
    "    # if len(tags) > 0: line += \"<\" + tags[0] + \">\"\n",
    "    # if len(tags) > 1: print(tags)\n",
    "    # print(line)\n",
    "    return line.strip(), tags, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = []\n",
    "targets = []\n",
    "\n",
    "targets = []\n",
    "oov = {}\n",
    "for DATA_FOLDER in TEST_DATA_FOLDERS if mode == \"test\" else TRAIN_DATA_FOLDERS:\n",
    "    for file in glob.glob(os.path.join(DATA_FOLDER, \"*.txt\")):\n",
    "        is_start = True\n",
    "        date, id, obj = os.path.basename(file)[:-4].split('_')[:3]\n",
    "        longid = date + \"_\" + id\n",
    "        year = date[:4]\n",
    "        \n",
    "        # if longid != \"20171128_03\": continue\n",
    "        \n",
    "        encoding = 'shift-jis'\n",
    "        if longid in ['20161205_03', '20161209_05', '20161215_04', '20161208_03', '20161205_07', '20161212_01', '20161209_04', \n",
    "                      '20161209_03', '20161208_01', '20161212_03', '20161208_02', '20161207_05', '20161215_03', '20161207_04',\n",
    "                     '20161205_04']: encoding = 'utf-8'\n",
    "        \n",
    "        lines = open(file, encoding=encoding).read().split('\\n')\n",
    "        # print(lines)\n",
    "\n",
    "        k = 0\n",
    "        while k < len(lines):\n",
    "            line = lines[k].strip()\n",
    "            if longid == '20161209_05':\n",
    "                start, end = line.split(' ')[1:3]\n",
    "            else:\n",
    "                start, end = line.split(' ')[1].split('-')\n",
    "            start, end = float(start) * 1000, float(end) * 1000\n",
    "            utt_id = line.split(' ')[0]\n",
    "\n",
    "            output_wav = os.path.join(OUTPUT_FOLDER, \"wav\", longid, \"%s_%s.wav\" % (utt_id, obj))\n",
    "            output_htk = os.path.join(OUTPUT_FOLDER, \"htk\", longid, \"%s_%s.htk\" % (utt_id, obj))\n",
    "            output_npy = os.path.join(OUTPUT_FOLDER, \"npy\", longid, \"%s_%s.npy\" % (utt_id, obj))\n",
    "            \n",
    "            if line[-2:] == obj + ':':\n",
    "                k += 1\n",
    "                s = []\n",
    "                tags = []\n",
    "                envs = set()\n",
    "                while k < len(lines):\n",
    "                    line = lines[k].strip()\n",
    "                    if line[-2:] == obj + ':': break\n",
    "\n",
    "                    line, tag, env = preproc(line)\n",
    "                    if env: envs.add(env)\n",
    "                    if tag: tags += tag\n",
    "                    if line != '': s.append(line)\n",
    "                    # if len(tag) > 0: s.append(\"<\" + tag[0] + \">\")\n",
    "                    k += 1\n",
    "                    \n",
    "                dtag = ['0'] * TAG_COUNT\n",
    "                for tag in tags:\n",
    "                    id = get_tag_id(tag)\n",
    "                    if id is not None: dtag[id] = '1'\n",
    "                \n",
    "                s = split_words(s, oov, False)\n",
    "                tags = '_'.join([';'.join(tag) for tag in tags])\n",
    "                \n",
    "                if len(s) >= 2 and float(end) - float(start) < 5000:\n",
    "                    #targets.append(' '.join([str(k) for k in s]))\n",
    "                    targets.append((longid, start, end, obj, ' '.join(dtag), ' '.join([str(k) for k in s]), is_start))\n",
    "                    is_start = False\n",
    "                    #targets.append(\"%s %07d %07d %s %s %s %s\" % (longid, start, end, utt_id, obj, ''.join(list(envs)) or '_', s))\n",
    "                    filepaths.append(output_npy)\n",
    "            else: k += 1\n",
    "\n",
    "ls = list(zip(targets, filepaths))\n",
    "ls.sort()\n",
    "targets, filepaths = zip(*ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterences: 10032\n"
     ]
    }
   ],
   "source": [
    "print(\"Utterences:\", len(filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 0\n",
      "[]\n",
      "OOV: 248\n",
      "[('ー', 590), ('致し', 13), ('ゾンビ', 11), ('ヤ', 9), ('らっしゃる', 9), ('高槻', 9), ('っ', 8), ('なくっ', 8), ('友禅', 8), ('等持院', 8), ('らっしゃい', 7), ('ご存知', 7), ('ガ', 6), ('かかっ', 6), ('烏丸', 6), ('下賀茂', 6), ('かなっ', 6), ('ヨ', 6), ('といった', 5), ('ネ', 5), ('上賀茂', 5), ('餃子', 5), ('。', 4), ('＋', 4), ('奴', 4), ('ジャガイモ', 4), ('リョ', 4), ('橿原', 4), ('メットガラ', 4), ('サムギョプサル', 4), ('ゅう', 4), ('就活', 3), ('ご覧', 3), ('ズ', 3), ('おしゃれ', 3), ('糺', 3), ('断', 3), ('ボーカロイド', 3), ('００７', 3), ('持', 3), ('ヒマラヤスギ', 3), ('日向', 3), ('試薬', 2), ('斎藤', 2), ('萌', 2), ('チョット', 2), ('露木', 2), ('ソウデスネ', 2), ('(', 2), ('惹か', 2), ('ガン', 2), ('採れ', 2), ('ジャ', 2), ('鋼', 2), ('錬金術', 2), ('紋', 2), ('\\u3000', 2), ('キョ', 2), ('ハッ', 2), ('橿原神宮', 2), ('混ん', 2), ('ソウネ', 2), ('ショーン・オブ・ザ・デッド', 2), ('オヤ', 2), ('信也', 2), ('匂い', 2), ('チュ', 2), ('智也', 2), ('コンピュータ', 2), ('１', 2), ('ガチ', 2), ('シネマ', 2), ('ラ・ラ・ランド', 2), ('オッケー', 2), ('ビッ', 2), ('ホッ', 2), ('祭典', 2), ('キョウ', 2), ('漬物', 2), ('チュパー', 2), ('タブレット', 2), ('濱田', 2), ('庄司', 2), ('才', 2), ('らっしゃっ', 2), ('スギ', 2), ('掃か', 2), ('ズク', 2), ('厨子', 2), ('沁み', 2), ('峽', 2), ('照', 2), ('椿山荘', 2), ('ゅうのか', 2), ('大慶', 1), ('哲也', 1), ('カンパニー', 1), ('懸け橋', 1), ('トートロジー', 1), ('ジュー', 1), ('セー', 1), ('断れ', 1), ('磨か', 1), ('断る', 1), ('仰せつかり', 1), ('充てる', 1), ('チヤ', 1), ('周遊', 1), ('願え', 1), ('購読', 1), ('ハヤ', 1), ('挿絵', 1), ('ダブルダッチ', 1), ('縮こまっ', 1), ('充分', 1), ('ありがたくっ', 1), ('ガードガ', 1), ('ヤッパ', 1), (')', 1), ('臨める', 1), ('腫瘍', 1), ('ヨユ', 1), ('トリュ', 1), ('シュー', 1), ('担える', 1), ('シュミレーションモデル', 1), ('シュミレーション', 1), ('挫け', 1), ('ダイガ', 1), ('シンギュラリティ', 1), ('D', 1), ('）', 1), ('オッシァエ', 1), ('理沙', 1), ('ナガ', 1), ('ナガラ', 1), ('滋養', 1), ('ヨシ', 1), ('きたっ', 1), ('恵', 1), ('沢山', 1), ('ガー', 1), ('ヨウ', 1), ('ょお', 1), ('浅くっ', 1), ('っきり', 1), ('聡', 1), ('定休', 1), ('捨離', 1), ('デシタッケ', 1), ('フネ', 1), ('ヤッテマシ', 1), ('チェ', 1), ('寝っ転がり', 1), ('デスカネ', 1), ('ドヴォルザーク', 1), ('ソット', 1), ('ケッコ', 1), ('アジャ', 1), ('ヒョ', 1), ('ナー', 1), ('タッ', 1), ('非常勤', 1), ('ツァ', 1), ('・', 1), ('パッ', 1), ('映さ', 1), ('ウズ', 1), ('ミッションインポッシブル', 1), ('コウヨウ', 1), ('ナルホドネ', 1), ('ツヨ', 1), ('ファッジ', 1), ('ケッコウ', 1), ('オンガクトカ', 1), ('多くっ', 1), ('ヒューマンズ', 1), ('スポーティ', 1), ('然', 1), ('さしかかっ', 1), ('詳し', 1), ('ヤム', 1), ('ッテ', 1), ('ガッ', 1), ('混む', 1), ('停', 1), ('延', 1), ('ドキュン', 1), ('チョッ', 1), ('チガ', 1), ('メット', 1), ('こもっ', 1), ('ヨイ', 1), ('アキ・カウリスマキ', 1), ('ドー', 1), ('トー', 1), ('秘話', 1), ('聡子', 1), ('鎮座', 1), ('ガチャガチャ', 1), ('経っ', 1), ('イヤ', 1), ('チュウガッ', 1), ('しんどくっ', 1), ('之', 1), ('介', 1), ('葵', 1), ('カヨ', 1), ('止ん', 1), ('眠たい', 1), ('ニー', 1), ('L', 1), ('診', 1), ('水越', 1), ('綺麗', 1), ('ワヤ', 1), ('釉薬', 1), ('オッシャ', 1), ('ゾ', 1), ('テンジョ', 1), ('パネ', 1), ('分っ', 1), ('置物', 1), ('ヒヤ', 1), ('コンズ', 1), ('キンジョニ', 1), ('ゅうのが', 1), ('ゅうような', 1), ('掃こ', 1), ('ゅうたら', 1), ('ゅうてね', 1), ('リッタ', 1), ('ギュー', 1), ('籠っ', 1), ('集っ', 1), ('ギリギリ', 1), ('如く', 1), ('空也', 1), ('弔う', 1), ('クリップ', 1), ('閉じこもっ', 1), ('鵜', 1), ('ジャージ', 1), ('ゅうのかな', 1), ('ゅうんかね', 1), ('ヒラガ', 1), ('ナガヤモシ', 1), ('カンヌ', 1)]\n"
     ]
    }
   ],
   "source": [
    "_vocab = { word: vocab[word] for word in vocab } if mode == \"train\" else vocab\n",
    "import operator\n",
    "sorted_words = sorted(_vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Vocab Size:\", len(sorted_words))\n",
    "print(sorted_words)\n",
    "\n",
    "print(\"OOV:\", len(oov))\n",
    "sorted_oov = sorted(oov.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_oov)\n",
    "\n",
    "if mode == \"train\":\n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'word_ids.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(['<unk> 0', '<sos> 1', '<eos> 2', '<sp> 4\\n']))\n",
    "        for i, word in enumerate(sorted_words): f.write('%s %d\\n' % (word[0], i + 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(filepaths)\n",
    "#print('\\n'.join([t for i, t in enumerate(targets)]))\n",
    "#with open('/n/sd7/trung/csp/data/erica/inputs.txt', 'w') as f:\n",
    "#    f.write('\\n'.join(filepaths))\n",
    "    \n",
    "#with open('/n/sd7/trung/csp/data/erica/targets.txt', 'w') as f:\n",
    "#    f.write('\\n'.join(t[4] for t in targets))\n",
    "    \n",
    "with open('/n/sd7/trung/csp/data/erica/%s_notag.txt' % mode, 'w') as f:\n",
    "    f.write('\\n'.join([path + ' ' + '2 ' + target[5] + ' 1' for path, target in zip(filepaths, targets)]))\n",
    "    \n",
    "with open('/n/sd7/trung/csp/data/erica/%s_notag_context.txt' % mode, 'w') as f:\n",
    "    for i in range(len(filepaths)):\n",
    "        if targets[i][6]:\n",
    "            f.write('%s\\t%s %s %s\\t2 %s 1' % (filepaths[i], '0', ' '.join('0' * (TAG_COUNT)), targets[i][4], targets[i][5]))\n",
    "        else:\n",
    "            f.write('%s\\t%s %s %s\\t2 %s 1' % (filepaths[i], ('0' if targets[i - 1][3] == targets[i][3] else '1'), targets[i - 1][4], targets[i][4], targets[i][5]))\n",
    "        if i != len(filepaths) - 1: f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav[test[0]:test[1]].export(\"/n/sd7/trung/test.wav\", format='wav')\n",
    "#IPython.display.Audio(\"/n/sd7/trung/test.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavfile = \"/n/sd7/trung/data/erica/htk/20171130_07/0041_U.htk\"\n",
    "#wavfile = wavfile.replace('htk', 'wav')\n",
    "#feat = get_features(wavfile)\n",
    "#print(feat)\n",
    "#IPython.display.Audio(wavfile, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
