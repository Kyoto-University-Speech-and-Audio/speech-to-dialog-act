{
  "name": "swda_decoder_output_5",
  "model": "da",
  "dataset": "swda",
  "input_unit": "word",
  "vocab_file": "data/swbd/vocab/words_20.txt",
  "train_data": "data/swbd/inputs_swda_padding25_speaker_norm_split20_train.txt",
  "dev_data": "data/swbd/inputs_swda_padding25_speaker_norm_split20_dev.txt",
  "test_data": "data/swbd/inputs_swda_padding25_speaker_norm_split20_test.txt",
  "predicted_train_data": "swda_padding25_out_train.csv",
  "predicted_dev_data": "swda_padding25_out_dev.csv",
  "predicted_test_data": "swda_padding25_out_test.csv",

  "learning_rate": 1e-3,
  "batch_size": 70,
  "eval_batch_size": 70,

  "da_input": "attention_context",
  "da_word_encoder_type": "bilstm",
  "num_da_word_encoder_layers": 2,
  "da_word_encoder_num_units": 256,
  
  "num_utt_history": 5,
  "num_da_classes": 43,
  "dropout": 0.0,
  
  "utt_encoder_num_units": 256,
  "embedding_size": 128,
  
  "learning_rate_start_decay_epoch": 4,
  "learning_rate_decay_steps": 1,
  "learning_rate_decay_rate": 0.5,

  "length_penalty_weight": 0.5,
  "max_gradient_norm": 5.0
}
